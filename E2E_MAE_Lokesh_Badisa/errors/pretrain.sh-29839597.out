[rank1]: Traceback (most recent call last):
[rank1]:   File "/global/u1/l/lokeshb/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 281, in <module>
[rank1]:     main(args)
[rank1]:   File "/global/u1/l/lokeshb/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 271, in main
[rank1]:     trainer = Trainer(model, train_loader, val_loader, loss_scaler, optimizer, args)
[rank1]:   File "/global/u1/l/lokeshb/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 114, in __init__
[rank1]:     self.model = DDP(model, device_ids=[torch.cuda.current_device()])
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py", line 810, in __init__
[rank1]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/utils.py", line 269, in _verify_param_shape_across_processes
[rank1]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank1]: torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2024, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
[rank1]: ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
[rank1]: Last error:
[rank1]: Duplicate GPU detected : rank 1 and rank 0 both on CUDA device 3000
[rank2]: Traceback (most recent call last):
[rank2]:   File "/global/u1/l/lokeshb/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 281, in <module>
[rank2]:     main(args)
[rank2]:   File "/global/u1/l/lokeshb/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 271, in main
[rank2]:     trainer = Trainer(model, train_loader, val_loader, loss_scaler, optimizer, args)
[rank2]:   File "/global/u1/l/lokeshb/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 114, in __init__
[rank2]:     self.model = DDP(model, device_ids=[torch.cuda.current_device()])
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py", line 810, in __init__
[rank2]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/utils.py", line 269, in _verify_param_shape_across_processes
[rank2]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank2]: torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2024, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
[rank2]: ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
[rank2]: Last error:
[rank2]: Duplicate GPU detected : rank 2 and rank 0 both on CUDA device 3000
[rank3]: Traceback (most recent call last):
[rank3]:   File "/global/u1/l/lokeshb/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 281, in <module>
[rank3]:     main(args)
[rank3]:   File "/global/u1/l/lokeshb/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 271, in main
[rank3]:     trainer = Trainer(model, train_loader, val_loader, loss_scaler, optimizer, args)
[rank3]:   File "/global/u1/l/lokeshb/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 114, in __init__
[rank3]:     self.model = DDP(model, device_ids=[torch.cuda.current_device()])
[rank3]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py", line 810, in __init__
[rank3]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank3]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/utils.py", line 269, in _verify_param_shape_across_processes
[rank3]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank3]: torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2024, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
[rank3]: ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
[rank3]: Last error:
[rank3]: Duplicate GPU detected : rank 3 and rank 0 both on CUDA device 3000
[rank0]: Traceback (most recent call last):
[rank0]:   File "/global/u1/l/lokeshb/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 281, in <module>
[rank0]:     main(args)
[rank0]:   File "/global/u1/l/lokeshb/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 271, in main
[rank0]:     trainer = Trainer(model, train_loader, val_loader, loss_scaler, optimizer, args)
[rank0]:   File "/global/u1/l/lokeshb/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 114, in __init__
[rank0]:     self.model = DDP(model, device_ids=[torch.cuda.current_device()])
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py", line 810, in __init__
[rank0]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/utils.py", line 269, in _verify_param_shape_across_processes
[rank0]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank0]: torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2024, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
[rank0]: ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
[rank0]: Last error:
[rank0]: Duplicate GPU detected : rank 0 and rank 1 both on CUDA device 3000
srun: error: nid001749: task 2: Exited with exit code 1
srun: Terminating StepId=29839597.0
slurmstepd: error: *** STEP 29839597.0 ON nid001749 CANCELLED AT 2024-08-27T05:52:35 ***
srun: error: nid001749: tasks 1,3: Exited with exit code 1
srun: error: nid001749: task 0: Exited with exit code 1

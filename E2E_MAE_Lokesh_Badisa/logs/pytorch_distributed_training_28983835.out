Node IP: 128.55.66.177
srun: Job 28983835 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=28983835.1
W0804 00:09:56.936000 140255904648064 torch/distributed/run.py:757] 
W0804 00:09:56.936000 140255904648064 torch/distributed/run.py:757] *****************************************
W0804 00:09:56.936000 140255904648064 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0804 00:09:56.936000 140255904648064 torch/distributed/run.py:757] *****************************************
I0804 00:09:56.937000 140255904648064 torch/distributed/launcher/api.py:188] Starting elastic_operator with launch configs:
I0804 00:09:56.937000 140255904648064 torch/distributed/launcher/api.py:188]   entrypoint       :  
I0804 00:09:56.937000 140255904648064 torch/distributed/launcher/api.py:188]   min_nodes        : 2
I0804 00:09:56.937000 140255904648064 torch/distributed/launcher/api.py:188]   max_nodes        : 2
I0804 00:09:56.937000 140255904648064 torch/distributed/launcher/api.py:188]   nproc_per_node   : 4
I0804 00:09:56.937000 140255904648064 torch/distributed/launcher/api.py:188]   run_id           : 31526
I0804 00:09:56.937000 140255904648064 torch/distributed/launcher/api.py:188]   rdzv_backend     : c10d
I0804 00:09:56.937000 140255904648064 torch/distributed/launcher/api.py:188]   rdzv_endpoint    : 128.55.66.177:29500
I0804 00:09:56.937000 140255904648064 torch/distributed/launcher/api.py:188]   rdzv_configs     : {'timeout': 900}
I0804 00:09:56.937000 140255904648064 torch/distributed/launcher/api.py:188]   max_restarts     : 0
I0804 00:09:56.937000 140255904648064 torch/distributed/launcher/api.py:188]   monitor_interval : 5
I0804 00:09:56.937000 140255904648064 torch/distributed/launcher/api.py:188]   log_dir          : /tmp/torchelastic_669jfnfe
I0804 00:09:56.937000 140255904648064 torch/distributed/launcher/api.py:188]   metrics_cfg      : {}
I0804 00:09:56.937000 140255904648064 torch/distributed/launcher/api.py:188] 
W0804 00:09:57.712000 140220598651776 torch/distributed/run.py:757] 
W0804 00:09:57.712000 140220598651776 torch/distributed/run.py:757] *****************************************
W0804 00:09:57.712000 140220598651776 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0804 00:09:57.712000 140220598651776 torch/distributed/run.py:757] *****************************************
I0804 00:09:57.712000 140220598651776 torch/distributed/launcher/api.py:188] Starting elastic_operator with launch configs:
I0804 00:09:57.712000 140220598651776 torch/distributed/launcher/api.py:188]   entrypoint       :  
I0804 00:09:57.712000 140220598651776 torch/distributed/launcher/api.py:188]   min_nodes        : 2
I0804 00:09:57.712000 140220598651776 torch/distributed/launcher/api.py:188]   max_nodes        : 2
I0804 00:09:57.712000 140220598651776 torch/distributed/launcher/api.py:188]   nproc_per_node   : 4
I0804 00:09:57.712000 140220598651776 torch/distributed/launcher/api.py:188]   run_id           : 31526
I0804 00:09:57.712000 140220598651776 torch/distributed/launcher/api.py:188]   rdzv_backend     : c10d
I0804 00:09:57.712000 140220598651776 torch/distributed/launcher/api.py:188]   rdzv_endpoint    : 128.55.66.177:29500
I0804 00:09:57.712000 140220598651776 torch/distributed/launcher/api.py:188]   rdzv_configs     : {'timeout': 900}
I0804 00:09:57.712000 140220598651776 torch/distributed/launcher/api.py:188]   max_restarts     : 0
I0804 00:09:57.712000 140220598651776 torch/distributed/launcher/api.py:188]   monitor_interval : 5
I0804 00:09:57.712000 140220598651776 torch/distributed/launcher/api.py:188]   log_dir          : /tmp/torchelastic_6enlxv3c
I0804 00:09:57.712000 140220598651776 torch/distributed/launcher/api.py:188]   metrics_cfg      : {}
I0804 00:09:57.712000 140220598651776 torch/distributed/launcher/api.py:188] 
I0804 00:09:57.725000 140220598651776 torch/distributed/elastic/agent/server/api.py:866] [default] starting workers for entrypoint: python3.10
I0804 00:09:57.725000 140220598651776 torch/distributed/elastic/agent/server/api.py:699] [default] Rendezvous'ing worker group
I0804 00:09:57.971000 140255904648064 torch/distributed/elastic/agent/server/api.py:866] [default] starting workers for entrypoint: python3.10
I0804 00:09:57.971000 140255904648064 torch/distributed/elastic/agent/server/api.py:699] [default] Rendezvous'ing worker group
I0804 00:09:59.395000 140220598651776 torch/distributed/elastic/agent/server/api.py:568] [default] Rendezvous complete for workers. Result:
I0804 00:09:59.395000 140220598651776 torch/distributed/elastic/agent/server/api.py:568]   restart_count=0
I0804 00:09:59.395000 140220598651776 torch/distributed/elastic/agent/server/api.py:568]   master_addr=nid001144-hsn0
I0804 00:09:59.395000 140220598651776 torch/distributed/elastic/agent/server/api.py:568]   master_port=40775
I0804 00:09:59.395000 140220598651776 torch/distributed/elastic/agent/server/api.py:568]   group_rank=0
I0804 00:09:59.395000 140220598651776 torch/distributed/elastic/agent/server/api.py:568]   group_world_size=2
I0804 00:09:59.395000 140220598651776 torch/distributed/elastic/agent/server/api.py:568]   local_ranks=[0, 1, 2, 3]
I0804 00:09:59.395000 140220598651776 torch/distributed/elastic/agent/server/api.py:568]   role_ranks=[0, 1, 2, 3]
I0804 00:09:59.395000 140220598651776 torch/distributed/elastic/agent/server/api.py:568]   global_ranks=[0, 1, 2, 3]
I0804 00:09:59.395000 140220598651776 torch/distributed/elastic/agent/server/api.py:568]   role_world_sizes=[8, 8, 8, 8]
I0804 00:09:59.395000 140220598651776 torch/distributed/elastic/agent/server/api.py:568]   global_world_sizes=[8, 8, 8, 8]
I0804 00:09:59.395000 140220598651776 torch/distributed/elastic/agent/server/api.py:568] 
I0804 00:09:59.395000 140255904648064 torch/distributed/elastic/agent/server/api.py:568] [default] Rendezvous complete for workers. Result:
I0804 00:09:59.395000 140255904648064 torch/distributed/elastic/agent/server/api.py:568]   restart_count=0
I0804 00:09:59.395000 140255904648064 torch/distributed/elastic/agent/server/api.py:568]   master_addr=nid001144-hsn0
I0804 00:09:59.395000 140255904648064 torch/distributed/elastic/agent/server/api.py:568]   master_port=40775
I0804 00:09:59.395000 140255904648064 torch/distributed/elastic/agent/server/api.py:568]   group_rank=1
I0804 00:09:59.395000 140255904648064 torch/distributed/elastic/agent/server/api.py:568]   group_world_size=2
I0804 00:09:59.395000 140255904648064 torch/distributed/elastic/agent/server/api.py:568]   local_ranks=[0, 1, 2, 3]
I0804 00:09:59.395000 140255904648064 torch/distributed/elastic/agent/server/api.py:568]   role_ranks=[4, 5, 6, 7]
I0804 00:09:59.395000 140220598651776 torch/distributed/elastic/agent/server/api.py:707] [default] Starting worker group
I0804 00:09:59.395000 140255904648064 torch/distributed/elastic/agent/server/api.py:568]   global_ranks=[4, 5, 6, 7]
I0804 00:09:59.395000 140255904648064 torch/distributed/elastic/agent/server/api.py:568]   role_world_sizes=[8, 8, 8, 8]
I0804 00:09:59.395000 140255904648064 torch/distributed/elastic/agent/server/api.py:568]   global_world_sizes=[8, 8, 8, 8]
I0804 00:09:59.395000 140255904648064 torch/distributed/elastic/agent/server/api.py:568] 
I0804 00:09:59.395000 140220598651776 torch/distributed/elastic/agent/server/local_elastic_agent.py:168] Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
I0804 00:09:59.395000 140255904648064 torch/distributed/elastic/agent/server/api.py:707] [default] Starting worker group
I0804 00:09:59.395000 140220598651776 torch/distributed/elastic/multiprocessing/api.py:263] log directory set to: /tmp/torchelastic_6enlxv3c/31526_89ovrxha
I0804 00:09:59.395000 140255904648064 torch/distributed/elastic/agent/server/local_elastic_agent.py:168] Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
I0804 00:09:59.395000 140220598651776 torch/distributed/elastic/multiprocessing/api.py:358] Setting worker0 reply file to: /tmp/torchelastic_6enlxv3c/31526_89ovrxha/attempt_0/0/error.json
I0804 00:09:59.396000 140255904648064 torch/distributed/elastic/multiprocessing/api.py:263] log directory set to: /tmp/torchelastic_669jfnfe/31526_f1xmbovn
I0804 00:09:59.396000 140220598651776 torch/distributed/elastic/multiprocessing/api.py:358] Setting worker1 reply file to: /tmp/torchelastic_6enlxv3c/31526_89ovrxha/attempt_0/1/error.json
I0804 00:09:59.396000 140255904648064 torch/distributed/elastic/multiprocessing/api.py:358] Setting worker0 reply file to: /tmp/torchelastic_669jfnfe/31526_f1xmbovn/attempt_0/0/error.json
I0804 00:09:59.396000 140220598651776 torch/distributed/elastic/multiprocessing/api.py:358] Setting worker2 reply file to: /tmp/torchelastic_6enlxv3c/31526_89ovrxha/attempt_0/2/error.json
I0804 00:09:59.396000 140255904648064 torch/distributed/elastic/multiprocessing/api.py:358] Setting worker1 reply file to: /tmp/torchelastic_669jfnfe/31526_f1xmbovn/attempt_0/1/error.json
I0804 00:09:59.396000 140220598651776 torch/distributed/elastic/multiprocessing/api.py:358] Setting worker3 reply file to: /tmp/torchelastic_6enlxv3c/31526_89ovrxha/attempt_0/3/error.json
I0804 00:09:59.396000 140255904648064 torch/distributed/elastic/multiprocessing/api.py:358] Setting worker2 reply file to: /tmp/torchelastic_669jfnfe/31526_f1xmbovn/attempt_0/2/error.json
I0804 00:09:59.396000 140255904648064 torch/distributed/elastic/multiprocessing/api.py:358] Setting worker3 reply file to: /tmp/torchelastic_669jfnfe/31526_f1xmbovn/attempt_0/3/error.json
/global/homes/l/lokeshb/.conda/envs/lokesh/bin/python3.10: can't open file '/global/u1/l/lokeshb/testing/E2E/E2E_MAE_Lokesh_Badisa/ ': [Errno 2] No such file or directory
/global/homes/l/lokeshb/.conda/envs/lokesh/bin/python3.10: can't open file '/global/u1/l/lokeshb/testing/E2E/E2E_MAE_Lokesh_Badisa/ ': [Errno 2] No such file or directory
/global/homes/l/lokeshb/.conda/envs/lokesh/bin/python3.10: can't open file '/global/u1/l/lokeshb/testing/E2E/E2E_MAE_Lokesh_Badisa/ ': [Errno 2] No such file or directory
/global/homes/l/lokeshb/.conda/envs/lokesh/bin/python3.10: can't open file '/global/u1/l/lokeshb/testing/E2E/E2E_MAE_Lokesh_Badisa/ ': [Errno 2] No such file or directory
/global/homes/l/lokeshb/.conda/envs/lokesh/bin/python3.10: can't open file '/global/u1/l/lokeshb/testing/E2E/E2E_MAE_Lokesh_Badisa/ ': [Errno 2] No such file or directory
/global/homes/l/lokeshb/.conda/envs/lokesh/bin/python3.10: can't open file '/global/u1/l/lokeshb/testing/E2E/E2E_MAE_Lokesh_Badisa/ ': [Errno 2] No such file or directory
/global/homes/l/lokeshb/.conda/envs/lokesh/bin/python3.10: can't open file '/global/u1/l/lokeshb/testing/E2E/E2E_MAE_Lokesh_Badisa/ ': [Errno 2] No such file or directory
/global/homes/l/lokeshb/.conda/envs/lokesh/bin/python3.10: can't open file '/global/u1/l/lokeshb/testing/E2E/E2E_MAE_Lokesh_Badisa/ ': [Errno 2] No such file or directory
E0804 00:10:04.409000 140220598651776 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 2) local_rank: 0 (pid: 1375587) of binary: /global/homes/l/lokeshb/.conda/envs/lokesh/bin/python3.10
E0804 00:10:04.413000 140255904648064 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 2) local_rank: 0 (pid: 1175284) of binary: /global/homes/l/lokeshb/.conda/envs/lokesh/bin/python3.10
I0804 00:10:04.563000 140220598651776 torch/distributed/elastic/multiprocessing/errors/__init__.py:360] ('local_rank %s FAILED with no error file. Decorate your entrypoint fn with @record for traceback info. See: https://pytorch.org/docs/stable/elastic/errors.html', 0)
Traceback (most recent call last):
  File "/global/homes/l/lokeshb/.conda/envs/lokesh/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/global/homes/l/lokeshb/.conda/envs/lokesh/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/global/homes/l/lokeshb/.conda/envs/lokesh/lib/python3.10/site-packages/torch/distributed/run.py", line 879, in main
    run(args)
  File "/global/homes/l/lokeshb/.conda/envs/lokesh/lib/python3.10/site-packages/torch/distributed/run.py", line 870, in run
    elastic_launch(
  File "/global/homes/l/lokeshb/.conda/envs/lokesh/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/global/homes/l/lokeshb/.conda/envs/lokesh/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
  FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-08-04_00:10:04
  host      : nid001144-hsn0
  rank      : 1 (local_rank: 1)
  exitcode  : 2 (pid: 1375588)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-08-04_00:10:04
  host      : nid001144-hsn0
  rank      : 2 (local_rank: 2)
  exitcode  : 2 (pid: 1375589)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-08-04_00:10:04
  host      : nid001144-hsn0
  rank      : 3 (local_rank: 3)
  exitcode  : 2 (pid: 1375590)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-08-04_00:10:04
  host      : nid001144-hsn0
  rank      : 0 (local_rank: 0)
  exitcode  : 2 (pid: 1375587)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
W0804 00:10:04.802000 140255904648064 torch/distributed/elastic/rendezvous/dynamic_rendezvous.py:1203] The node 'nid008369-hsn0_1175245_0' has failed to shutdown the rendezvous '31526' due to an error of type RendezvousConnectionError.
W0804 00:10:04.826000 140255904648064 torch/distributed/elastic/rendezvous/dynamic_rendezvous.py:1203] The node 'nid008369-hsn0_1175245_0' has failed to shutdown the rendezvous '31526' due to an error of type RendezvousConnectionError.
W0804 00:10:04.844000 140255904648064 torch/distributed/elastic/rendezvous/dynamic_rendezvous.py:1203] The node 'nid008369-hsn0_1175245_0' has failed to shutdown the rendezvous '31526' due to an error of type RendezvousConnectionError.
I0804 00:10:04.844000 140255904648064 torch/distributed/elastic/multiprocessing/errors/__init__.py:360] ('local_rank %s FAILED with no error file. Decorate your entrypoint fn with @record for traceback info. See: https://pytorch.org/docs/stable/elastic/errors.html', 4)
Traceback (most recent call last):
  File "/global/homes/l/lokeshb/.conda/envs/lokesh/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/global/homes/l/lokeshb/.conda/envs/lokesh/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/global/homes/l/lokeshb/.conda/envs/lokesh/lib/python3.10/site-packages/torch/distributed/run.py", line 879, in main
    run(args)
  File "/global/homes/l/lokeshb/.conda/envs/lokesh/lib/python3.10/site-packages/torch/distributed/run.py", line 870, in run
    elastic_launch(
  File "/global/homes/l/lokeshb/.conda/envs/lokesh/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/global/homes/l/lokeshb/.conda/envs/lokesh/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
  FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-08-04_00:10:04
  host      : nid008369-hsn0
  rank      : 5 (local_rank: 1)
  exitcode  : 2 (pid: 1175285)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-08-04_00:10:04
  host      : nid008369-hsn0
  rank      : 6 (local_rank: 2)
  exitcode  : 2 (pid: 1175286)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-08-04_00:10:04
  host      : nid008369-hsn0
  rank      : 7 (local_rank: 3)
  exitcode  : 2 (pid: 1175287)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-08-04_00:10:04
  host      : nid008369-hsn0
  rank      : 4 (local_rank: 0)
  exitcode  : 2 (pid: 1175284)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: nid001144: task 0: Exited with exit code 1
srun: Terminating StepId=28983835.1
srun: error: nid008369: task 1: Exited with exit code 1
Traceback (most recent call last):
  File "/global/u1/l/lokeshb/testing/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 7, in <module>
    from model import ViTMAE
  File "/global/u1/l/lokeshb/testing/E2E/E2E_MAE_Lokesh_Badisa/model.py", line 9, in <module>
    from utils.misc import get_gauss
  File "/global/u1/l/lokeshb/testing/E2E/E2E_MAE_Lokesh_Badisa/utils/misc.py", line 44, in <module>
    sync_file = _get_sync_file()
  File "/global/u1/l/lokeshb/testing/E2E/E2E_MAE_Lokesh_Badisa/utils/misc.py", line 41, in _get_sync_file
    sync_file_dir, os.environ['SLURM_JOB_ID'], os.environ['SLURM_STEP_ID'])
  File "/global/homes/l/lokeshb/.conda/envs/lokesh/lib/python3.10/os.py", line 680, in __getitem__
    raise KeyError(key) from None
KeyError: 'SLURM_STEP_ID'

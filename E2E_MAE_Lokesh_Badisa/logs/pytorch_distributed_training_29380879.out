WORLD_SIZE: 4, GLOBAL_RANK: 2
Local Rank: 2
WORLD_SIZE: 4, GLOBAL_RANK: 0
Local Rank: 0
WORLD_SIZE: 4, GLOBAL_RANK: 1
Local Rank: 1
WORLD_SIZE: 4, GLOBAL_RANK: 3
Local Rank: 3
[rank0]: Traceback (most recent call last):
[rank0]:   File "/global/u1/l/lokeshb/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 278, in <module>
[rank0]:     main(args)
[rank0]:   File "/global/u1/l/lokeshb/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 269, in main
[rank0]:     trainer = Trainer(model, train_loader, val_loader, loss_scaler, optimizer, args)
[rank0]:   File "/global/u1/l/lokeshb/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 114, in __init__
[rank0]:     self.model = DDP(model, device_ids=[torch.cuda.current_device()])
[rank0]:   File "/global/homes/l/lokeshb/.conda/envs/lokesh/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 798, in __init__
[rank0]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank0]:   File "/global/homes/l/lokeshb/.conda/envs/lokesh/lib/python3.10/site-packages/torch/distributed/utils.py", line 269, in _verify_param_shape_across_processes
[rank0]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank0]: torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1970, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.20.5
[rank0]: ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank0]: Last error:
[rank0]: nvmlDeviceGetHandleByPciBusId() failed: Not Found
[rank3]: Traceback (most recent call last):
[rank3]:   File "/global/u1/l/lokeshb/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 278, in <module>
[rank3]:     main(args)
[rank3]:   File "/global/u1/l/lokeshb/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 269, in main
[rank3]:     trainer = Trainer(model, train_loader, val_loader, loss_scaler, optimizer, args)
[rank3]:   File "/global/u1/l/lokeshb/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 114, in __init__
[rank3]:     self.model = DDP(model, device_ids=[torch.cuda.current_device()])
[rank3]:   File "/global/homes/l/lokeshb/.conda/envs/lokesh/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 798, in __init__
[rank3]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank3]:   File "/global/homes/l/lokeshb/.conda/envs/lokesh/lib/python3.10/site-packages/torch/distributed/utils.py", line 269, in _verify_param_shape_across_processes
[rank3]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank3]: torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1970, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.20.5
[rank3]: ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank3]: Last error:
[rank3]: nvmlDeviceGetHandleByPciBusId() failed: Not Found
[rank2]: Traceback (most recent call last):
[rank2]:   File "/global/u1/l/lokeshb/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 278, in <module>
[rank2]:     main(args)
[rank2]:   File "/global/u1/l/lokeshb/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 269, in main
[rank2]:     trainer = Trainer(model, train_loader, val_loader, loss_scaler, optimizer, args)
[rank2]:   File "/global/u1/l/lokeshb/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 114, in __init__
[rank2]:     self.model = DDP(model, device_ids=[torch.cuda.current_device()])
[rank2]:   File "/global/homes/l/lokeshb/.conda/envs/lokesh/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 798, in __init__
[rank2]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank2]:   File "/global/homes/l/lokeshb/.conda/envs/lokesh/lib/python3.10/site-packages/torch/distributed/utils.py", line 269, in _verify_param_shape_across_processes
[rank2]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank2]: torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1970, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.20.5
[rank2]: ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank2]: Last error:
[rank2]: nvmlDeviceGetHandleByPciBusId() failed: Not Found
[rank1]: Traceback (most recent call last):
[rank1]:   File "/global/u1/l/lokeshb/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 278, in <module>
[rank1]:     main(args)
[rank1]:   File "/global/u1/l/lokeshb/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 269, in main
[rank1]:     trainer = Trainer(model, train_loader, val_loader, loss_scaler, optimizer, args)
[rank1]:   File "/global/u1/l/lokeshb/E2E/E2E_MAE_Lokesh_Badisa/dist-training.py", line 114, in __init__
[rank1]:     self.model = DDP(model, device_ids=[torch.cuda.current_device()])
[rank1]:   File "/global/homes/l/lokeshb/.conda/envs/lokesh/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 798, in __init__
[rank1]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank1]:   File "/global/homes/l/lokeshb/.conda/envs/lokesh/lib/python3.10/site-packages/torch/distributed/utils.py", line 269, in _verify_param_shape_across_processes
[rank1]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank1]: torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1970, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.20.5
[rank1]: ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank1]: Last error:
[rank1]: nvmlDeviceGetHandleByPciBusId() failed: Not Found
srun: error: nid001113: tasks 0,3: Exited with exit code 1
srun: Terminating StepId=29380879.0
slurmstepd: error: *** STEP 29380879.0 ON nid001113 CANCELLED AT 2024-08-14T16:06:23 ***
srun: error: nid001113: tasks 1-2: Exited with exit code 1

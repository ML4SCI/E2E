{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "855e67aa-63a2-4dc4-bd48-b13a51566516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import ctypes\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import dataset\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn import metrics\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "\n",
    "os.environ['PJRT_DEVICE'] = 'TPU' \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "libc = ctypes.CDLL(\"libc.so.6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc45cd8c-858b-4210-8b96-cfebc46d81a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bffb6f3d-c8f2-436b-9e92-78a8cc63b690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [02:50<00:00, 585.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import gc\n",
    "from PIL import Image\n",
    "X_Pretrain = []\n",
    "for i in tqdm(range(100000)):\n",
    "    with Image.open(f'/home/shashank/Extracted/0/{i+1}.png') as img:\n",
    "        img_arr = np.array(img).reshape((125,125,8))\n",
    "        X_Pretrain.append(img_arr)\n",
    "        \n",
    "    with Image.open(f'/home/shashank/Extracted/1/{i+1}.png') as img:\n",
    "        img_arr = np.array(img).reshape((125,125,8))\n",
    "        X_Pretrain.append(img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f30ff605-36a4-4e8c-8c58-9644246c6141",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(X_Pretrain)\n",
    "X_train = X_Pretrain[:int(0.8*length)]\n",
    "X_test = X_Pretrain[int(0.8*length): int(length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b9164d5-c443-45b8-b48a-faf720f333f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "# Initialize lists to hold means and stds for each channel\n",
    "mean = []\n",
    "std = []\n",
    "\n",
    "# Calculate mean and std for each channel\n",
    "for i in range(X_train.shape[-1]):  # Assuming the last dimension is the channel\n",
    "    mean.append(X_train[:, :, :, i].mean())\n",
    "    std.append(X_train[:, :, :, i].std())\n",
    "\n",
    "# Normalize each channel\n",
    "for i in range(X_train.shape[-1]):\n",
    "    X_train[:, :, :, i] = (X_train[:, :, :, i] - mean[i]) / std[i]\n",
    "    X_test[:, :, :, i] = (X_test[:, :, :, i] - mean[i]) / std[i]\n",
    "\n",
    "# Perform zero suppression\n",
    "X_train[X_train < 1e-3] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ef2d1e-0714-4452-b695-846fbb32d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype='float32')\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from timm.models.vision_transformer import PatchEmbed, Block\n",
    "\n",
    "class Transformer_block(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=8,\n",
    "                 embed_dim=1024, depth=24, num_heads=16,\n",
    "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE encoder specifics\n",
    "        self.mask_ratio = 0.75\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        \n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        w = self.patch_embed.proj.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        \n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 8, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "        h = w = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 8, h, p, w, p))\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 8))\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 8, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        h = w = int(x.shape[1]**.5)\n",
    "        assert h * w == x.shape[1]\n",
    "        \n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 8))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 8, h * p, h * p))\n",
    "        return imgs\n",
    "\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        \n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "#         imgs = self.patchify(x)\n",
    "\n",
    "        \n",
    "        # embed patches\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "        # masking: length -> length * mask_ratio\n",
    "        x, mask, ids_restore = self.random_masking(x, self.mask_ratio)\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x, mask, ids_restore\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=8,\n",
    "                 embed_dim=1024, depth=24, num_heads=16,\n",
    "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block = Transformer_block(img_size=img_size, patch_size=patch_size, in_chans=in_chans,\n",
    "                 embed_dim=embed_dim, depth=depth, num_heads=num_heads,\n",
    "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded_tensor = []\n",
    "        masks = []\n",
    "        ids = []\n",
    "        \n",
    "        for i in range(8):\n",
    "            img = x[:,i,:,:].unsqueeze(1)\n",
    "            op, mask, ids_restore = self.block(img)\n",
    "            encoded_tensor.append(op)\n",
    "            masks.append(mask)\n",
    "            ids.append(ids_restore)\n",
    "            \n",
    "        img = self.block.patchify(x)\n",
    "            \n",
    "        return encoded_tensor, masks, ids, img\n",
    "    \n",
    "import torchvision\n",
    "from torchview import draw_graph\n",
    "from torchviz import make_dot\n",
    "encoder = Encoder(\n",
    "    img_size=125, patch_size=5, in_chans = 1,embed_dim=96, depth=2, num_heads=8,\n",
    "    mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
    "\n",
    "# model_graph = draw_graph(encoder, input_size=(1,8,125,125), expand_nested=True)\n",
    "# model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fee081f-79e5-4f1e-a4a2-f5ef65753763",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_transformer_block(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=8,\n",
    "                 embed_dim=1024, depth=24, num_heads=16,\n",
    "                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_patches = (img_size//patch_size)**2\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * 8, bias=True) # decoder to patch\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "\n",
    "        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.num_patches**.5), cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 8, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "        h = w = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 8, h, p, w, p))\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 8))\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 8, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        h = w = int(x.shape[1]**.5)\n",
    "        assert h * w == x.shape[1]\n",
    "        \n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 8))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 8, h * p, h * p))\n",
    "        return imgs\n",
    "\n",
    "    def forward(self, x):\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "#         mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "#         x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "#         x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "#         x =   # append cls token\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=8,\n",
    "             embed_dim=1024, depth=24, num_heads=16,\n",
    "             decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "             mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block = Decoder_transformer_block(img_size=img_size, patch_size=patch_size, in_chans=in_chans,\n",
    "                                                 embed_dim=embed_dim, depth=depth, num_heads=num_heads,\n",
    "                                                 decoder_embed_dim=decoder_embed_dim, decoder_depth=decoder_depth, decoder_num_heads=decoder_num_heads,\n",
    "                                                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False)\n",
    "        \n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, 96))\n",
    "        \n",
    "    def forward(self, x, ids):\n",
    "        \n",
    "        op = []\n",
    "        cls_token = []\n",
    "        for i in range(len(x)):\n",
    "#             print(x[i].shape)\n",
    "            mask_tokens = self.mask_token.repeat(x[i].shape[0], ids[i].shape[1] + 1 - x[i].shape[1], 1)\n",
    "            x_ = torch.cat([x[i][:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "            cls_token.append(x[i][:,:1,:])\n",
    "            x_ = torch.gather(x_, dim=1, index=ids[i].unsqueeze(-1).repeat(1, 1, x[i].shape[2]))  # unshuffle\n",
    "            op.append(x_)\n",
    "    \n",
    "        op = torch.cat(op, axis = 2)\n",
    "        cls_token = torch.cat(cls_token, axis = 2)\n",
    "#         print(op.shape)\n",
    "        op = torch.cat([cls_token, op], dim=1)\n",
    "#         print(op.shape)\n",
    "        op = self.block(op)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10095934-0ab7-43e2-a690-cc682191901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Masked_VIT(nn.Module):\n",
    "    def __init__(self, encoder, decoder, mask_ratio):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.mask_ratio = mask_ratio\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, mask, ids_restore, img = self.encoder(x)\n",
    "        pred = self.decoder(x, ids_restore)\n",
    "        \n",
    "        return pred, mask, img\n",
    "    \n",
    "def mae_vit_base_patch16_dec512d8b(img_size=125, mask_ratio = 0.75, **kwargs):\n",
    "    encoder = Encoder(\n",
    "        img_size=img_size, patch_size=5, embed_dim=96, depth=10, num_heads=8,in_chans = 1,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    \n",
    "    decoder = Decoder(\n",
    "        img_size=img_size, patch_size=5, embed_dim=768, depth=4, num_heads=8,in_chans = 1,\n",
    "        decoder_embed_dim=512, decoder_depth=2, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    \n",
    "    model = Masked_VIT(encoder, decoder, mask_ratio)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the model\n",
    "model = mae_vit_base_patch16_dec512d8b(img_size=125, mask_ratio = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d0c1ea2-1feb-4dcd-b56c-22a824c62dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "# from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "class Custom_Dataset(Dataset):\n",
    "    def __init__(self, x, transform):\n",
    "        self.x = x\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_1 = (self.x[idx]).astype('float32')\n",
    "        \n",
    "        if self.transform:\n",
    "            img_1 = self.transform(img_1)\n",
    "            \n",
    "        sample = {'img' : img_1}\n",
    "        \n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7d837d8-75a6-4475-ac29-1bd3326bb1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (160000, 125, 125, 8)\n",
      "Sample image shape: torch.Size([8, 125, 125])\n",
      "Sample image values: tensor(85.)\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "dataset = Custom_Dataset(X_train, transform = transform)\n",
    "sample = dataset.__getitem__(0)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Sample image shape:\", sample['img'].shape)\n",
    "print(\"Sample image values:\", sample['img'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58f127bc-40fd-499b-80f4-fab934377684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['img'].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "960c6e50-186b-48ea-a31e-e7bf3c0cf685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_fn(vals):\n",
    "    # take average\n",
    "    return sum(vals) / len(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b9b801c-8aa0-45ed-837b-67a33a015bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_dataloader, valid_dataloader, criterion, scheduler, device, optimizer):\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "\n",
    "    model.train()\n",
    "    scheduler.step()\n",
    "\n",
    "    for step, batch in (enumerate(train_dataloader)):\n",
    "        image = batch['img'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs, ind, target = model(image)\n",
    "        loss = criterion(target, outputs, ind)\n",
    "        loss.backward()\n",
    "        xm.optimizer_step(optimizer)\n",
    "        \n",
    "        if step%1000 == 0:\n",
    "            xm.master_print(f'Train_Batch: {step}, loss: {loss.item()}')\n",
    "\n",
    "        loss_reduced = xm.mesh_reduce('loss_reduce',loss,reduce_fn) \n",
    "        train_loss.append(loss_reduced.detach().cpu().numpy())\n",
    "        gc.collect()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in (enumerate(valid_dataloader)):\n",
    "            image = batch['img'].to(device)\n",
    "            outputs, ind, target = model(image)\n",
    "            loss = criterion(target, outputs, ind)\n",
    "            loss_reduced = xm.mesh_reduce('loss_reduce',loss,reduce_fn) \n",
    "            valid_loss.append(loss_reduced.detach().cpu().numpy())\n",
    "            \n",
    "            if step%500 == 0:\n",
    "                xm.master_print(f'Train_Batch: {step}, loss: {loss.item()}')\n",
    "            gc.collect()\n",
    "\n",
    "    return np.mean(train_loss), np.mean(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f35a180-bfa2-4a3c-bd35-cc03e4a2575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    ## Train dataset transformations\n",
    "    train_transform = transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.RandomVerticalFlip(),\n",
    "                                transforms.RandomRotation(60),])\n",
    "\n",
    "    ## Test dataset transformations\n",
    "    test_transform = transforms.Compose([\n",
    "                                transforms.ToTensor(),])\n",
    "\n",
    "    ##Train Dataset\n",
    "    train_dataset = Custom_Dataset(X_train, transform = train_transform)\n",
    "    \n",
    "    ##Test Dataset\n",
    "    test_dataset = Custom_Dataset(X_test, transform = test_transform)\n",
    "    \n",
    "    ##Sampler\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "                                                                    train_dataset,\n",
    "                                                                    num_replicas=xm.xrt_world_size(),\n",
    "                                                                    rank=xm.get_ordinal(),\n",
    "                                                                    shuffle = True\n",
    "                                                                    )\n",
    "    \n",
    "    ##Train Dataloader\n",
    "    dataloader_train = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                    batch_size=64,\n",
    "                                                    sampler = train_sampler,\n",
    "                                                    drop_last = True,\n",
    "                                                    num_workers=1,\n",
    "                                                    pin_memory = True)\n",
    "    \n",
    "    \n",
    "    ##Sampler\n",
    "    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "                                                                    test_dataset,\n",
    "                                                                    num_replicas=xm.xrt_world_size(),\n",
    "                                                                    rank=xm.get_ordinal(),\n",
    "                                                                    shuffle = False\n",
    "                                                                    )\n",
    "    ##Test Dataloader\n",
    "    dataloader_test = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                  sampler = valid_sampler,\n",
    "                                                  batch_size=64,\n",
    "                                                  drop_last = True,\n",
    "                                                  num_workers=1,\n",
    "                                                 )\n",
    "    return dataloader_train, dataloader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4ab4066-2231-472f-a8af-a2fe420252e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "def custom_loss(imgs, pred, mask):\n",
    "    imgs = imgs.reshape((imgs.shape[0], imgs.shape[1], 8, imgs.shape[2]//8))\n",
    "    pred = pred.reshape((pred.shape[0], pred.shape[1], 8, pred.shape[2]//8))\n",
    "    L = 0\n",
    "    for i in range(len(mask)):\n",
    "        l = ((pred[:, : ,i, :] - imgs[:, : ,i, :])**2).unsqueeze(axis = 2).mean(axis = -1)\n",
    "        mask_cpu = mask[i].unsqueeze(axis = -1)\n",
    "        loss = (l * mask_cpu).sum()\n",
    "        L+=((loss / mask_cpu.sum()))\n",
    "    return L/8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3babb286-673f-4651-9ec7-b5fe878de6dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Unsupported nprocs (8), ignoring...\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1718045162.979280  237243 pjrt_api.cc:100] GetPjrtApi was found for tpu at /home/shashank/.local/lib/python3.10/site-packages/libtpu/libtpu.so\n",
      "I0000 00:00:1718045162.979277  237245 pjrt_api.cc:100] GetPjrtApi was found for tpu at /home/shashank/.local/lib/python3.10/site-packages/libtpu/libtpu.so\n",
      "I0000 00:00:1718045162.979370  237243 pjrt_api.cc:79] PJRT_Api is set for device type tpu\n",
      "I0000 00:00:1718045162.979376  237245 pjrt_api.cc:79] PJRT_Api is set for device type tpu\n",
      "I0000 00:00:1718045162.979377  237243 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\n",
      "I0000 00:00:1718045162.979384  237245 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1718045162.987262  237247 pjrt_api.cc:100] GetPjrtApi was found for tpu at /home/shashank/.local/lib/python3.10/site-packages/libtpu/libtpu.so\n",
      "I0000 00:00:1718045162.987359  237247 pjrt_api.cc:79] PJRT_Api is set for device type tpu\n",
      "I0000 00:00:1718045162.987366  237247 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1718045162.995229  237246 pjrt_api.cc:100] GetPjrtApi was found for tpu at /home/shashank/.local/lib/python3.10/site-packages/libtpu/libtpu.so\n",
      "I0000 00:00:1718045162.995315  237246 pjrt_api.cc:79] PJRT_Api is set for device type tpu\n",
      "I0000 00:00:1718045162.995322  237246 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_training_steps = 31250, world_size=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                        | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 2.443760871887207\n",
      "Train_Batch: 0, loss: 0.9238588809967041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▏                                                                                                         | 1/50 [15:53<12:58:42, 953.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.8941, Val Loss: 0.9370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▏                                                                                                         | 1/50 [15:53<12:58:48, 953.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.8961636424064636\n",
      "Train_Batch: 0, loss: 0.8579180240631104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|████▎                                                                                                       | 2/50 [28:08<11:00:01, 825.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Train Loss: 0.8382, Val Loss: 0.8791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|████▎                                                                                                       | 2/50 [28:08<11:00:03, 825.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.8436738848686218\n",
      "Train_Batch: 0, loss: 0.820464015007019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██████▍                                                                                                     | 3/50 [40:02<10:06:26, 774.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Train Loss: 0.7986, Val Loss: 0.8461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██████▍                                                                                                     | 3/50 [40:02<10:06:27, 774.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.8159753680229187\n",
      "Train_Batch: 0, loss: 0.7562995553016663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|████████▋                                                                                                    | 4/50 [52:23<9:43:31, 761.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Train Loss: 0.7723, Val Loss: 0.7633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|████████▋                                                                                                    | 4/50 [52:23<9:43:33, 761.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.712538480758667\n",
      "Train_Batch: 0, loss: 0.6479917764663696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████▋                                                                                                | 5/50 [1:04:36<9:23:16, 751.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Train Loss: 0.6534, Val Loss: 0.6649"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████▋                                                                                                | 5/50 [1:04:36<9:23:16, 751.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████▋                                                                                                | 5/50 [1:04:36<9:23:16, 751.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.6256807446479797\n",
      "Train_Batch: 0, loss: 0.6311755180358887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████████████▊                                                                                              | 6/50 [1:17:00<9:08:53, 748.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Train Loss: 0.6105, Val Loss: 0.6524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████████████▊                                                                                              | 6/50 [1:17:00<9:08:56, 748.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.6063635349273682\n",
      "Train_Batch: 0, loss: 0.927241861820221\n",
      "Epoch 7/50, Train Loss: 0.6621, Val Loss: 0.9381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|██████████████▉                                                                                            | 7/50 [1:29:05<8:50:59, 740.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.9052706360816956\n",
      "Train_Batch: 0, loss: 0.9347460269927979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█████████████████                                                                                          | 8/50 [1:41:01<8:33:07, 733.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Train Loss: 0.8767, Val Loss: 0.9368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█████████████████                                                                                          | 8/50 [1:41:01<8:33:07, 733.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.9013424515724182\n",
      "Train_Batch: 0, loss: 0.8318907022476196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████████████████▎                                                                                       | 9/50 [1:53:26<8:23:29, 736.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Train Loss: 0.8441, Val Loss: 0.8638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████████████████▎                                                                                       | 9/50 [1:53:26<8:23:31, 736.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.8592588305473328\n",
      "Train_Batch: 0, loss: 0.6570556163787842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████████▏                                                                                    | 10/50 [2:05:45<8:11:42, 737.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Train Loss: 0.7283, Val Loss: 0.6732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████████▏                                                                                    | 10/50 [2:05:46<8:11:46, 737.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.6564249992370605\n",
      "Train_Batch: 0, loss: 0.8257248997688293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|███████████████████████▎                                                                                  | 11/50 [2:17:47<7:56:09, 732.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Train Loss: 0.7995, Val Loss: 0.8643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|███████████████████████▎                                                                                  | 11/50 [2:17:47<7:56:09, 732.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.8460394144058228\n",
      "Train_Batch: 0, loss: 0.7589004635810852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████████████████████████▍                                                                                | 12/50 [2:29:45<7:41:13, 728.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Train Loss: 0.7673, Val Loss: 0.7647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████████████████████████▍                                                                                | 12/50 [2:29:45<7:41:13, 728.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.7316854596138\n",
      "Train_Batch: 0, loss: 0.7555913329124451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|███████████████████████████▌                                                                              | 13/50 [2:41:36<7:25:50, 722.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Train Loss: 0.6351, Val Loss: 0.7609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|███████████████████████████▌                                                                              | 13/50 [2:41:36<7:25:52, 723.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.7120552659034729\n",
      "Train_Batch: 0, loss: 0.6458893418312073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|█████████████████████████████▋                                                                            | 14/50 [2:53:48<7:15:23, 725.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, Train Loss: 0.6242, Val Loss: 0.6649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|█████████████████████████████▋                                                                            | 14/50 [2:53:48<7:15:24, 725.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.6158808469772339\n",
      "Train_Batch: 0, loss: 0.6360694169998169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████████████▊                                                                          | 15/50 [3:05:50<7:02:42, 724.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, Train Loss: 0.6130, Val Loss: 0.6564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████████████▊                                                                          | 15/50 [3:05:50<7:02:42, 724.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.6121924519538879\n",
      "Train_Batch: 0, loss: 0.6474121809005737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████████████████████████▉                                                                        | 16/50 [3:17:40<6:48:06, 720.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, Train Loss: 0.6070, Val Loss: 0.6523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████████████████████████▉                                                                        | 16/50 [3:17:40<6:48:06, 720.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.5949216485023499\n",
      "Train_Batch: 0, loss: 0.6326241493225098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|████████████████████████████████████                                                                      | 17/50 [3:29:23<6:33:20, 715.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, Train Loss: 0.6020, Val Loss: 0.6499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|████████████████████████████████████                                                                      | 17/50 [3:29:23<6:33:21, 715.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.6133371591567993\n",
      "Train_Batch: 0, loss: 0.6204537749290466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|██████████████████████████████████████▏                                                                   | 18/50 [3:41:08<6:19:46, 712.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, Train Loss: 0.5995, Val Loss: 0.6469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|██████████████████████████████████████▏                                                                   | 18/50 [3:41:09<6:19:49, 712.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.6171403527259827\n",
      "Train_Batch: 0, loss: 0.8399602174758911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|████████████████████████████████████████▎                                                                 | 19/50 [3:52:59<6:07:43, 711.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, Train Loss: 0.6980, Val Loss: 0.8748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|████████████████████████████████████████▎                                                                 | 19/50 [3:52:59<6:07:42, 711.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.8428778648376465\n",
      "Train_Batch: 0, loss: 0.8365665674209595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████████▍                                                               | 20/50 [4:05:05<5:57:55, 715.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Train Loss: 0.8125, Val Loss: 0.8563"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████████▍                                                               | 20/50 [4:05:05<5:57:56, 715.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████████▍                                                               | 20/50 [4:05:05<5:57:55, 715.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.8149753212928772\n",
      "Train_Batch: 0, loss: 0.6247141361236572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████████████████████████████████████████████▌                                                             | 21/50 [4:16:56<5:45:21, 714.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, Train Loss: 0.7319, Val Loss: 0.6722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████████████████████████████████████████████▌                                                             | 21/50 [4:16:56<5:45:22, 714.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.6358078718185425\n",
      "Train_Batch: 0, loss: 0.9190616607666016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████████████████████████▋                                                           | 22/50 [4:29:01<5:34:52, 717.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, Train Loss: 0.6974, Val Loss: 0.9316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████████████████████████▋                                                           | 22/50 [4:29:01<5:34:53, 717.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.9176903963088989\n",
      "Train_Batch: 0, loss: 0.8212273120880127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████████████████████████████████████████████████▊                                                         | 23/50 [4:41:17<5:25:26, 723.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, Train Loss: 0.8341, Val Loss: 0.8576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████████████████████████████████████████████████▊                                                         | 23/50 [4:41:17<5:25:25, 723.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.8093144297599792\n",
      "Train_Batch: 0, loss: 0.6432429552078247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|██████████████████████████████████████████████████▉                                                       | 24/50 [4:52:48<5:09:12, 713.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, Train Loss: 0.6939, Val Loss: 0.6698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|██████████████████████████████████████████████████▉                                                       | 24/50 [4:52:48<5:09:12, 713.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.6226403117179871\n",
      "Train_Batch: 0, loss: 0.6381670832633972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████                                                     | 25/50 [5:04:26<4:55:22, 708.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, Train Loss: 0.6213, Val Loss: 0.6648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████                                                     | 25/50 [5:04:26<4:55:22, 708.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.6269721984863281\n",
      "Train_Batch: 0, loss: 0.6207976341247559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|███████████████████████████████████████████████████████                                                   | 26/50 [5:16:40<4:46:28, 716.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, Train Loss: 0.6147, Val Loss: 0.6543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|███████████████████████████████████████████████████████                                                   | 26/50 [5:16:40<4:46:30, 716.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.6025993824005127\n",
      "Train_Batch: 0, loss: 0.6416395306587219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████████████████████████████████████████████████████████▏                                                | 27/50 [5:28:46<4:35:41, 719.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, Train Loss: 0.6060, Val Loss: 0.6516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████████████████████████████████████████████████████████▏                                                | 27/50 [5:28:46<4:35:40, 719.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.6013585925102234\n",
      "Train_Batch: 0, loss: 0.6263825297355652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|███████████████████████████████████████████████████████████▎                                              | 28/50 [5:40:38<4:22:59, 717.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, Train Loss: 0.6015, Val Loss: 0.6483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|███████████████████████████████████████████████████████████▎                                              | 28/50 [5:40:38<4:22:58, 717.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.6076695919036865\n",
      "Train_Batch: 0, loss: 0.6332429647445679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████████████████████████████████████████████████████████████▍                                            | 29/50 [5:52:47<4:12:16, 720.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, Train Loss: 0.5984, Val Loss: 0.6470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████████████████████████████████████████████████████████████▍                                            | 29/50 [5:52:48<4:12:17, 720.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.6371113657951355\n",
      "Train_Batch: 0, loss: 0.9060326218605042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████████▌                                          | 30/50 [6:04:55<4:00:58, 722.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Train Loss: 0.6150, Val Loss: 0.9357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████████▌                                          | 30/50 [6:04:56<4:00:59, 722.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.9219942688941956\n",
      "Train_Batch: 0, loss: 0.9062083959579468\n",
      "Epoch 31/50, Train Loss: 0.8916, Val Loss: 0.9370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|█████████████████████████████████████████████████████████████████▋                                        | 31/50 [6:17:06<3:49:37, 725.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.9018923044204712\n",
      "Train_Batch: 0, loss: 0.8506211638450623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|███████████████████████████████████████████████████████████████████▊                                      | 32/50 [6:29:16<3:37:59, 726.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, Train Loss: 0.8261, Val Loss: 0.8524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|███████████████████████████████████████████████████████████████████▊                                      | 32/50 [6:29:16<3:37:59, 726.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.8147609233856201\n",
      "Train_Batch: 0, loss: 0.8276975750923157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|█████████████████████████████████████████████████████████████████████▉                                    | 33/50 [6:41:31<3:26:38, 729.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50, Train Loss: 0.8017, Val Loss: 0.8488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|█████████████████████████████████████████████████████████████████████▉                                    | 33/50 [6:41:31<3:26:38, 729.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.8222281336784363\n",
      "Train_Batch: 0, loss: 0.9208526611328125\n",
      "Epoch 34/50, Train Loss: 0.8509, Val Loss: 0.9381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|████████████████████████████████████████████████████████████████████████                                  | 34/50 [6:53:30<3:13:37, 726.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.8997282385826111\n",
      "Train_Batch: 0, loss: 0.9455628395080566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████████████████████▏                               | 35/50 [7:04:59<2:58:46, 715.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50, Train Loss: 0.8771, Val Loss: 0.9373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████████████████████▏                               | 35/50 [7:04:59<2:58:46, 715.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.8886680603027344\n",
      "Train_Batch: 0, loss: 0.8104318380355835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|████████████████████████████████████████████████████████████████████████████▎                             | 36/50 [7:17:23<2:48:51, 723.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50, Train Loss: 0.8596, Val Loss: 0.8648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|████████████████████████████████████████████████████████████████████████████▎                             | 36/50 [7:17:23<2:48:52, 723.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.8162357211112976\n",
      "Train_Batch: 0, loss: 0.8309673070907593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|██████████████████████████████████████████████████████████████████████████████▍                           | 37/50 [7:29:27<2:36:47, 723.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50, Train Loss: 0.8132, Val Loss: 0.8519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|██████████████████████████████████████████████████████████████████████████████▍                           | 37/50 [7:29:27<2:36:47, 723.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.8463197946548462\n",
      "Train_Batch: 0, loss: 0.8234573602676392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|████████████████████████████████████████████████████████████████████████████████▌                         | 38/50 [7:41:33<2:24:54, 724.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50, Train Loss: 0.7993, Val Loss: 0.8485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|████████████████████████████████████████████████████████████████████████████████▌                         | 38/50 [7:41:33<2:24:54, 724.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.8237121105194092\n",
      "Train_Batch: 0, loss: 0.8301070332527161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|██████████████████████████████████████████████████████████████████████████████████▋                       | 39/50 [7:53:39<2:12:52, 724.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50, Train Loss: 0.7960, Val Loss: 0.8446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|██████████████████████████████████████████████████████████████████████████████████▋                       | 39/50 [7:53:39<2:12:52, 724.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.824735164642334\n",
      "Train_Batch: 0, loss: 0.8498616218566895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████████████████████████████████▊                     | 40/50 [8:05:55<2:01:23, 728.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Train Loss: 0.7948, Val Loss: 0.8373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████████████████████████████████▊                     | 40/50 [8:05:56<2:01:24, 728.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.8164457082748413\n",
      "Train_Batch: 0, loss: 0.7883502244949341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|██████████████████████████████████████████████████████████████████████████████████████▉                   | 41/50 [8:18:06<1:49:21, 729.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50, Train Loss: 0.7970, Val Loss: 0.8108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|██████████████████████████████████████████████████████████████████████████████████████▉                   | 41/50 [8:18:06<1:49:21, 729.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.7717388868331909\n",
      "Train_Batch: 0, loss: 0.6521676778793335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|█████████████████████████████████████████████████████████████████████████████████████████                 | 42/50 [8:29:57<1:36:28, 723.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50, Train Loss: 0.6924, Val Loss: 0.6795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|█████████████████████████████████████████████████████████████████████████████████████████                 | 42/50 [8:29:57<1:36:28, 723.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.6423434019088745\n",
      "Train_Batch: 0, loss: 0.6681905388832092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|███████████████████████████████████████████████████████████████████████████████████████████▏              | 43/50 [8:42:09<1:24:43, 726.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50, Train Loss: 0.6289, Val Loss: 0.6681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|███████████████████████████████████████████████████████████████████████████████████████████▏              | 43/50 [8:42:09<1:24:43, 726.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.659909188747406\n",
      "Train_Batch: 0, loss: 0.635234534740448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████████████████████████████████████████████▎            | 44/50 [8:54:22<1:12:48, 728.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50, Train Loss: 0.6198, Val Loss: 0.6635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████████████████████████████████████████████▎            | 44/50 [8:54:22<1:12:48, 728.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.6560507416725159\n",
      "Train_Batch: 0, loss: 0.625282883644104\n",
      "Epoch 45/50, Train Loss: 0.6102, Val Loss: 0.6535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████████████████████████████████████████████████████████████▍          | 45/50 [9:06:48<1:01:07, 733.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.599724292755127\n",
      "Train_Batch: 0, loss: 0.6246556043624878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████████████████████████████████████████████████████████████████▎        | 46/50 [9:19:09<49:03, 735.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50, Train Loss: 0.6038, Val Loss: 0.6501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████████████████████████████████████████████████████████████████▎        | 46/50 [9:19:09<49:03, 735.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.6085877418518066\n",
      "Train_Batch: 0, loss: 0.6262769103050232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌      | 47/50 [9:30:40<36:06, 722.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50, Train Loss: 0.5998, Val Loss: 0.6492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌      | 47/50 [9:30:40<36:06, 722.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.5911676287651062\n",
      "Train_Batch: 0, loss: 0.6108502745628357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████▋    | 48/50 [9:42:28<23:56, 718.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50, Train Loss: 0.5964, Val Loss: 0.6471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████▋    | 48/50 [9:42:28<23:56, 718.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.615562379360199\n",
      "Train_Batch: 0, loss: 0.6341506242752075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 49/50 [9:54:30<11:59, 719.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50, Train Loss: 0.5933, Val Loss: 0.6453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 49/50 [9:54:30<11:59, 719.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.6057142019271851\n",
      "Train_Batch: 0, loss: 0.6356748938560486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [10:06:42<00:00, 728.04s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [10:06:42<00:00, 728.04s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [10:06:42<00:00, 728.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.5914, Val Loss: 0.6439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [10:06:42<00:00, 728.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_function(model, epochs):\n",
    "    criterion = custom_loss\n",
    "    lr = 0.001\n",
    "    num_train_steps = int(\n",
    "        len(X_train) / 64 / xm.xrt_world_size() * epochs\n",
    "    )\n",
    "\n",
    "    lr = lr * xm.xrt_world_size()\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.05)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "    train_dataloader, test_dataloader = data()\n",
    "    \n",
    "    xm.master_print(f'num_training_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n",
    "    device = xm.xla_device()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        gc.collect()\n",
    "        para_loader = pl.ParallelLoader(train_dataloader, [device])\n",
    "        train_loader = para_loader.per_device_loader(device)\n",
    "\n",
    "        val_loader = pl.ParallelLoader(test_dataloader, [device])\n",
    "        valid_loader = val_loader.per_device_loader(device)\n",
    "        \n",
    "        trn_loss, val_loss = train_one_epoch(model, train_loader, valid_loader, criterion, scheduler, device, optimizer)\n",
    "        \n",
    "        scheduler.step()\n",
    "        train_loss.append(trn_loss)\n",
    "        valid_loss.append(val_loss)\n",
    "        gc.collect()\n",
    "\n",
    "        xm.master_print(f'Epoch {epoch+1}/{epochs}, Train Loss: {trn_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        with open('losses.txt', 'a') as f:\n",
    "            f.write(f'Epoch {epoch+1}/{epochs}, Train Loss: {trn_loss:.4f}, Val Loss: {val_loss:.4f}\\n')\n",
    "        \n",
    "    xm.rendezvous('save_model')\n",
    "    xm.master_print('save model')\n",
    "    xm.save(model.state_dict(), f'./Model_files/xla_trained_model_epoch.pth')\n",
    "\n",
    "def _mp_fn(rank, flags):\n",
    "    try:\n",
    "        model = mae_vit_base_patch16_dec512d8b(img_size=125, mask_ratio=0.75)\n",
    "        dev = xm.xla_device()\n",
    "        model = model.to(dev)\n",
    "        dataloader_train, dataloader_test = data()\n",
    "        torch.set_default_tensor_type('torch.FloatTensor')\n",
    "        train_function(model, epochs=50)\n",
    "        # xser.save(model.state_dict(), f\"model.bin\", master_only=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Exception in process {rank}: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    FLAGS = {}\n",
    "    try:\n",
    "        xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')\n",
    "    except Exception as e:\n",
    "        print(f\"Exception in main: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a898673-0e14-4bbf-bb40-e71672f7c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = mae_vit_base_patch16_dec512d8b(img_size=125, mask_ratio = 0.75)\n",
    "# a = train_function(model, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88499da3-415e-4d36-8e8f-068af833b849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _mp_fn(rank, flags):\n",
    "#     model = mae_vit_base_patch16_dec512d8b(img_size=125, mask_ratio = 0.75)\n",
    "#     train_function(model, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebb4a05f-3fed-415f-a0ac-c8d6cd13a77f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     FLAGS={}\n",
    "#     xmp.spawn(_mp_fn, args=(FLAGS,), start_method='fork')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77142c88-54c8-46de-b6ef-ae2379f574f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1718081571.815271  224454 pjrt_api.cc:100] GetPjrtApi was found for tpu at /home/shashank/.local/lib/python3.10/site-packages/libtpu/libtpu.so\n",
      "I0000 00:00:1718081571.815347  224454 pjrt_api.cc:79] PJRT_Api is set for device type tpu\n",
      "I0000 00:00:1718081571.815352  224454 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\n"
     ]
    }
   ],
   "source": [
    "xm.save(model.encoder.state_dict(), './Model_files/encoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4dac7c5b-0d1b-4554-9365-b298118c5e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./Model_files/xla_trained_model_epoch.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74fe10de-f1d9-4f6a-a60b-b0c18699311c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dev \u001b[38;5;241m=\u001b[39m xm\u001b[38;5;241m.\u001b[39mxla_device()\n\u001b[0;32m----> 2\u001b[0m imgs, pred, ind \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m pred\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munpatchify\u001b[39m(x):\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "dev = xm.xla_device()\n",
    "imgs, pred, ind = model(sample['img'].unsqueeze(0).to(dev))\n",
    "\n",
    "pred.shape\n",
    "\n",
    "def unpatchify(x):\n",
    "    \"\"\"\n",
    "    x: (N, L, patch_size**2 *3)\n",
    "    imgs: (N, 8, H, W)\n",
    "    \"\"\"\n",
    "    p = 5\n",
    "    h = w = int(x.shape[1]**.5)\n",
    "    assert h * w == x.shape[1]\n",
    "    \n",
    "    x = x.reshape(shape=(x.shape[0], h, w, p, p, 8))\n",
    "    x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "    imgs = x.reshape(shape=(x.shape[0], 8, h * p, h * p))\n",
    "    return imgs\n",
    "\n",
    "# pred = pred.reshape((125,125,8))\n",
    "pred = unpatchify(pred)\n",
    "pred.shape\n",
    "pred = pred.reshape((8, 125, 125))\n",
    "pred.shape\n",
    "\n",
    "img = pred.permute(1,2,0)\n",
    "img = img.cpu().detach().numpy()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(8):\n",
    "    plt.imshow(img[:,:,i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc70ce18-a2e4-4938-ab1b-9fdc09d0f00c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb32d26-7716-4450-bc06-68037fd0015c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36cd16-89c0-4316-ad10-c24001c7dddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab8dec0-5df8-4137-9bd4-1ff821c71b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

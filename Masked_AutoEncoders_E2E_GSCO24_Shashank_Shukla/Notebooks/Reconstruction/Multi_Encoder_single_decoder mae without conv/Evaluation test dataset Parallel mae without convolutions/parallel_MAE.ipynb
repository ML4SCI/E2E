{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "855e67aa-63a2-4dc4-bd48-b13a51566516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import ctypes\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import dataset\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn import metrics\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "\n",
    "os.environ['PJRT_DEVICE'] = 'TPU' \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "libc = ctypes.CDLL(\"libc.so.6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc45cd8c-858b-4210-8b96-cfebc46d81a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bffb6f3d-c8f2-436b-9e92-78a8cc63b690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:17<00:00, 572.16it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import gc\n",
    "from PIL import Image\n",
    "X_Pretrain = []\n",
    "for i in tqdm(range(10000)):\n",
    "    with Image.open(f'/home/shashank/Extracted/0/{i+1}.png') as img:\n",
    "        img_arr = np.array(img).reshape((125,125,8))\n",
    "        X_Pretrain.append(img_arr)\n",
    "        \n",
    "    with Image.open(f'/home/shashank/Extracted/1/{i+1}.png') as img:\n",
    "        img_arr = np.array(img).reshape((125,125,8))\n",
    "        X_Pretrain.append(img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f30ff605-36a4-4e8c-8c58-9644246c6141",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(X_Pretrain)\n",
    "X_train = X_Pretrain[:int(0.8*length)]\n",
    "X_test = X_Pretrain[int(0.8*length): int(length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b9164d5-c443-45b8-b48a-faf720f333f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "# Initialize lists to hold means and stds for each channel\n",
    "mean = []\n",
    "std = []\n",
    "\n",
    "# Calculate mean and std for each channel\n",
    "for i in range(X_train.shape[-1]):  # Assuming the last dimension is the channel\n",
    "    mean.append(X_train[:, :, :, i].mean())\n",
    "    std.append(X_train[:, :, :, i].std())\n",
    "\n",
    "# Normalize each channel\n",
    "for i in range(X_train.shape[-1]):\n",
    "    X_train[:, :, :, i] = (X_train[:, :, :, i] - mean[i]) / std[i]\n",
    "    X_test[:, :, :, i] = (X_test[:, :, :, i] - mean[i]) / std[i]\n",
    "\n",
    "# Perform zero suppression\n",
    "X_train[X_train < 1e-3] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ef2d1e-0714-4452-b695-846fbb32d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype='float32')\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from timm.models.vision_transformer import PatchEmbed, Block\n",
    "\n",
    "class Transformer_block(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=8,\n",
    "                 embed_dim=1024, depth=24, num_heads=16,\n",
    "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE encoder specifics\n",
    "        self.mask_ratio = 0.75\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        \n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        w = self.patch_embed.proj.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        \n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 8, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "        h = w = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 8, h, p, w, p))\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 8))\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 8, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        h = w = int(x.shape[1]**.5)\n",
    "        assert h * w == x.shape[1]\n",
    "        \n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 8))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 8, h * p, h * p))\n",
    "        return imgs\n",
    "\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        \n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "#         imgs = self.patchify(x)\n",
    "\n",
    "        \n",
    "        # embed patches\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "        # masking: length -> length * mask_ratio\n",
    "        x, mask, ids_restore = self.random_masking(x, self.mask_ratio)\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x, mask, ids_restore\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=8,\n",
    "                 embed_dim=1024, depth=24, num_heads=16,\n",
    "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            Transformer_block(img_size=img_size, patch_size=patch_size, in_chans=1,\n",
    "                 embed_dim=embed_dim, depth=depth, num_heads=num_heads,\n",
    "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False)\n",
    "            for _ in range(8)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded_tensor = []\n",
    "        masks = []\n",
    "        ids = []\n",
    "        \n",
    "        for i in range(8):\n",
    "            img = x[:, i, :, :].unsqueeze(1)\n",
    "            op, mask, ids_restore = self.blocks[i](img)\n",
    "            encoded_tensor.append(op)\n",
    "            masks.append(mask)\n",
    "            ids.append(ids_restore)\n",
    "            \n",
    "        img = self.blocks[0].patchify(x)  # Assuming patchify is the same for all blocks\n",
    "            \n",
    "        return encoded_tensor, masks, ids, img\n",
    "    \n",
    "import torchvision\n",
    "from torchview import draw_graph\n",
    "from torchviz import make_dot\n",
    "encoder = Encoder(\n",
    "    img_size=125, patch_size=5, in_chans = 1,embed_dim=96, depth=2, num_heads=8,\n",
    "    mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
    "\n",
    "# model_graph = draw_graph(encoder, input_size=(1,8,125,125), expand_nested=True)\n",
    "# model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fee081f-79e5-4f1e-a4a2-f5ef65753763",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_transformer_block(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=8,\n",
    "                 embed_dim=1024, depth=24, num_heads=16,\n",
    "                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_patches = (img_size//patch_size)**2\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * 8, bias=True) # decoder to patch\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "\n",
    "        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.num_patches**.5), cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 8, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "        h = w = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 8, h, p, w, p))\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 8))\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 8, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        h = w = int(x.shape[1]**.5)\n",
    "        assert h * w == x.shape[1]\n",
    "        \n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 8))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 8, h * p, h * p))\n",
    "        return imgs\n",
    "\n",
    "    def forward(self, x):\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "#         mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "#         x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "#         x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "#         x =   # append cls token\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=8,\n",
    "             embed_dim=1024, depth=24, num_heads=16,\n",
    "             decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "             mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block = Decoder_transformer_block(img_size=img_size, patch_size=patch_size, in_chans=in_chans,\n",
    "                                                 embed_dim=embed_dim, depth=depth, num_heads=num_heads,\n",
    "                                                 decoder_embed_dim=decoder_embed_dim, decoder_depth=decoder_depth, decoder_num_heads=decoder_num_heads,\n",
    "                                                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False)\n",
    "        \n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, 96))\n",
    "        \n",
    "    def forward(self, x, ids):\n",
    "        \n",
    "        op = []\n",
    "        cls_token = []\n",
    "        for i in range(len(x)):\n",
    "#             print(x[i].shape)\n",
    "            mask_tokens = self.mask_token.repeat(x[i].shape[0], ids[i].shape[1] + 1 - x[i].shape[1], 1)\n",
    "            x_ = torch.cat([x[i][:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "            cls_token.append(x[i][:,:1,:])\n",
    "            x_ = torch.gather(x_, dim=1, index=ids[i].unsqueeze(-1).repeat(1, 1, x[i].shape[2]))  # unshuffle\n",
    "            op.append(x_)\n",
    "    \n",
    "        op = torch.cat(op, axis = 2)\n",
    "        cls_token = torch.cat(cls_token, axis = 2)\n",
    "#         print(op.shape)\n",
    "        op = torch.cat([cls_token, op], dim=1)\n",
    "#         print(op.shape)\n",
    "        op = self.block(op)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10095934-0ab7-43e2-a690-cc682191901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Masked_VIT(nn.Module):\n",
    "    def __init__(self, encoder, decoder, mask_ratio):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.mask_ratio = mask_ratio\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, mask, ids_restore, img = self.encoder(x)\n",
    "        pred = self.decoder(x, ids_restore)\n",
    "        \n",
    "        return pred, mask, img\n",
    "    \n",
    "def mae_vit_base_patch16_dec512d8b(img_size=125, mask_ratio = 0.75, **kwargs):\n",
    "    encoder = Encoder(\n",
    "        img_size=img_size, patch_size=5, embed_dim=96, depth=10, num_heads=8,in_chans = 1,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    \n",
    "    decoder = Decoder(\n",
    "        img_size=img_size, patch_size=5, embed_dim=768, depth=4, num_heads=8,in_chans = 1,\n",
    "        decoder_embed_dim=512, decoder_depth=2, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    \n",
    "    model = Masked_VIT(encoder, decoder, mask_ratio)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the model\n",
    "model = mae_vit_base_patch16_dec512d8b(img_size=125, mask_ratio = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d0c1ea2-1feb-4dcd-b56c-22a824c62dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "# from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "class Custom_Dataset(Dataset):\n",
    "    def __init__(self, x, transform):\n",
    "        self.x = x\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_1 = (self.x[idx]).astype('float32')\n",
    "        \n",
    "        if self.transform:\n",
    "            img_1 = self.transform(img_1)\n",
    "            \n",
    "        sample = {'img' : img_1}\n",
    "        \n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7d837d8-75a6-4475-ac29-1bd3326bb1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (16000, 125, 125, 8)\n",
      "Sample image shape: torch.Size([8, 125, 125])\n",
      "Sample image values: tensor(85.)\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "dataset = Custom_Dataset(X_train, transform = transform)\n",
    "sample = dataset.__getitem__(0)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Sample image shape:\", sample['img'].shape)\n",
    "print(\"Sample image values:\", sample['img'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e34cb211-90bc-4134-980d-386643be2b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# sample['img'].max()\n",
    "\n",
    "# op, mask, img = model(sample['img'].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "867d3d8d-adad-4b87-822f-eb532c49f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "def custom_loss(imgs, pred, mask):\n",
    "    imgs = imgs.reshape((imgs.shape[0], imgs.shape[1], 8, imgs.shape[2]//8))\n",
    "    pred = pred.reshape((pred.shape[0], pred.shape[1], 8, pred.shape[2]//8))\n",
    "    L = 0\n",
    "    for i in range(len(mask)):\n",
    "        l = ((pred[:, : ,i, :] - imgs[:, : ,i, :])**2).unsqueeze(axis = 2).mean(axis = -1)\n",
    "        mask_cpu = mask[i].unsqueeze(axis = -1)\n",
    "        loss = (l * mask_cpu).sum()\n",
    "        L+=((loss / mask_cpu.sum()))\n",
    "    return L/8\n",
    "\n",
    "# print(custom_loss(img, op, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "960c6e50-186b-48ea-a31e-e7bf3c0cf685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_fn(vals):\n",
    "    # take average\n",
    "    # xm.print(len(vals))\n",
    "    return sum(vals) / len(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b9b801c-8aa0-45ed-837b-67a33a015bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_dataloader, valid_dataloader, criterion, scheduler, device, optimizer):\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "\n",
    "    model.train()\n",
    "    scheduler.step()\n",
    "\n",
    "    for step, batch in (enumerate(train_dataloader)):\n",
    "        image = batch['img'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs, ind, target = model(image)\n",
    "        loss = criterion(target, outputs, ind)\n",
    "        loss.backward()\n",
    "        xm.optimizer_step(optimizer)\n",
    "        \n",
    "        if step%100 == 0:\n",
    "            xm.master_print(f'Train_Batch: {step}, loss: {loss.item()}')\n",
    "        \n",
    "        loss_reduced = xm.mesh_reduce('loss_reduce',loss,reduce_fn) \n",
    "        # if loss.item() == np.nan or loss_reduced == np.nan or loss.item() == 'nan' or loss_reduced == 'nan':\n",
    "        #     xm.print('nan here')\n",
    "        train_loss.append(loss_reduced.detach().cpu().numpy())\n",
    "        gc.collect()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in (enumerate(valid_dataloader)):\n",
    "            image = batch['img'].to(device)\n",
    "            outputs, ind, target = model(image)\n",
    "            loss = criterion(target, outputs, ind)\n",
    "            loss_reduced = xm.mesh_reduce('loss_reduce',loss,reduce_fn) \n",
    "            valid_loss.append(loss_reduced.detach().cpu().numpy())\n",
    "            \n",
    "            if step%500 == 0:\n",
    "                xm.master_print(f'Train_Batch: {step}, loss: {loss.item()}')\n",
    "            gc.collect()\n",
    "\n",
    "    return np.mean(train_loss), np.mean(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f35a180-bfa2-4a3c-bd35-cc03e4a2575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    ## Train dataset transformations\n",
    "    train_transform = transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.RandomVerticalFlip(),\n",
    "                                transforms.RandomRotation(60),])\n",
    "\n",
    "    ## Test dataset transformations\n",
    "    test_transform = transforms.Compose([\n",
    "                                transforms.ToTensor(),])\n",
    "\n",
    "    ##Train Dataset\n",
    "    train_dataset = Custom_Dataset(X_train, transform = train_transform)\n",
    "    \n",
    "    ##Test Dataset\n",
    "    test_dataset = Custom_Dataset(X_test, transform = test_transform)\n",
    "    \n",
    "    ##Sampler\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "                                                                    train_dataset,\n",
    "                                                                    num_replicas=xm.xrt_world_size(),\n",
    "                                                                    rank=xm.get_ordinal(),\n",
    "                                                                    shuffle = True\n",
    "                                                                    )\n",
    "    \n",
    "    ##Train Dataloader\n",
    "    dataloader_train = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                    batch_size=64,\n",
    "                                                    sampler = train_sampler,\n",
    "                                                    drop_last = True,\n",
    "                                                    num_workers=1,\n",
    "                                                    pin_memory = True)\n",
    "    \n",
    "    \n",
    "    ##Sampler\n",
    "    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "                                                                    test_dataset,\n",
    "                                                                    num_replicas=xm.xrt_world_size(),\n",
    "                                                                    rank=xm.get_ordinal(),\n",
    "                                                                    shuffle = False\n",
    "                                                                    )\n",
    "    ##Test Dataloader\n",
    "    dataloader_test = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                  sampler = valid_sampler,\n",
    "                                                  batch_size=64,\n",
    "                                                  drop_last = True,\n",
    "                                                  num_workers=1,\n",
    "                                                 )\n",
    "    return dataloader_train, dataloader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ab4066-2231-472f-a8af-a2fe420252e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3babb286-673f-4651-9ec7-b5fe878de6dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Unsupported nprocs (8), ignoring...\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1718216586.340628  132323 pjrt_api.cc:100] GetPjrtApi was found for tpu at /home/shashank/.local/lib/python3.10/site-packages/libtpu/libtpu.so\n",
      "I0000 00:00:1718216586.340629  132325 pjrt_api.cc:100] GetPjrtApi was found for tpu at /home/shashank/.local/lib/python3.10/site-packages/libtpu/libtpu.so\n",
      "I0000 00:00:1718216586.340629  132326 pjrt_api.cc:100] GetPjrtApi was found for tpu at /home/shashank/.local/lib/python3.10/site-packages/libtpu/libtpu.so\n",
      "I0000 00:00:1718216586.340629  132327 pjrt_api.cc:100] GetPjrtApi was found for tpu at /home/shashank/.local/lib/python3.10/site-packages/libtpu/libtpu.so\n",
      "I0000 00:00:1718216586.340737  132323 pjrt_api.cc:79] PJRT_Api is set for device type tpu\n",
      "I0000 00:00:1718216586.340743  132326 pjrt_api.cc:79] PJRT_Api is set for device type tpu\n",
      "I0000 00:00:1718216586.340743  132325 pjrt_api.cc:79] PJRT_Api is set for device type tpu\n",
      "I0000 00:00:1718216586.340743  132327 pjrt_api.cc:79] PJRT_Api is set for device type tpu\n",
      "I0000 00:00:1718216586.340744  132323 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\n",
      "I0000 00:00:1718216586.340750  132326 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\n",
      "I0000 00:00:1718216586.340753  132327 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\n",
      "I0000 00:00:1718216586.340753  132325 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\n",
      "  0%|                                                                                                                                                      | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_training_steps = 3125, world_size=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                      | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 2.4530189037323\n",
      "Train_Batch: 0, loss: 0.863929271697998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▊                                                                                                                                       | 1/50 [12:33<10:15:30, 753.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 1.0527, Val Loss: 0.9378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▊                                                                                                                                       | 1/50 [12:34<10:15:53, 754.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.8816561698913574\n",
      "Train_Batch: 0, loss: 0.8656580448150635\n",
      "Epoch 2/50, Train Loss: 0.8782, Val Loss: 0.9403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█████▌                                                                                                                                     | 2/50 [17:59<6:41:45, 502.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.865944504737854\n",
      "Train_Batch: 0, loss: 0.8497732877731323\n",
      "Epoch 3/50, Train Loss: 0.8779, Val Loss: 0.9399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████████▎                                                                                                                                  | 3/50 [23:21<5:28:44, 419.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.888867199420929\n",
      "Train_Batch: 0, loss: 0.8620827794075012\n",
      "Epoch 4/50, Train Loss: 0.8753, Val Loss: 0.9416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███████████                                                                                                                                | 4/50 [28:43<4:52:08, 381.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.887859046459198\n",
      "Train_Batch: 0, loss: 0.8771802186965942\n",
      "Epoch 5/50, Train Loss: 0.8760, Val Loss: 0.9438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████▉                                                                                                                             | 5/50 [33:58<4:27:56, 357.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.8577214479446411\n",
      "Train_Batch: 0, loss: 0.8409002423286438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████████████████▋                                                                                                                          | 6/50 [39:13<4:11:34, 343.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Train Loss: 0.8773, Val Loss: 0.9412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████████████████▋                                                                                                                          | 6/50 [39:14<4:11:38, 343.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.8801209330558777\n",
      "Train_Batch: 0, loss: 0.8531301617622375\n",
      "Epoch 7/50, Train Loss: 0.8766, Val Loss: 0.9422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████████████████▍                                                                                                                       | 7/50 [44:31<4:00:00, 334.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.8842013478279114\n",
      "Train_Batch: 0, loss: 0.8585719466209412\n",
      "Epoch 8/50, Train Loss: 0.8780, Val Loss: 0.9387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████████████████████▏                                                                                                                    | 8/50 [49:50<3:50:48, 329.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.8712243437767029\n",
      "Train_Batch: 0, loss: 0.8604073524475098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█████████████████████████                                                                                                                  | 9/50 [55:05<3:42:13, 325.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Train Loss: 0.8782, Val Loss: 0.9403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█████████████████████████                                                                                                                  | 9/50 [55:05<3:42:14, 325.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Batch: 0, loss: 0.8674978613853455\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_function(model, epochs):\n",
    "    criterion = custom_loss\n",
    "    lr = 0.001\n",
    "    num_train_steps = int(\n",
    "        len(X_train) / 64 / xm.xrt_world_size() * epochs\n",
    "    )\n",
    "\n",
    "    lr = lr * xm.xrt_world_size()\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.05)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "    train_dataloader, test_dataloader = data()\n",
    "    \n",
    "    xm.master_print(f'num_training_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n",
    "    device = xm.xla_device()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        gc.collect()\n",
    "        para_loader = pl.ParallelLoader(train_dataloader, [device])\n",
    "        train_loader = para_loader.per_device_loader(device)\n",
    "\n",
    "        val_loader = pl.ParallelLoader(test_dataloader, [device])\n",
    "        valid_loader = val_loader.per_device_loader(device)\n",
    "        \n",
    "        trn_loss, val_loss = train_one_epoch(model, train_loader, valid_loader, criterion, scheduler, device, optimizer)\n",
    "        \n",
    "        scheduler.step()\n",
    "        train_loss.append(trn_loss)\n",
    "        valid_loss.append(val_loss)\n",
    "        gc.collect()\n",
    "\n",
    "        xm.master_print(f'Epoch {epoch+1}/{epochs}, Train Loss: {trn_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        with open('losses.txt', 'a') as f:\n",
    "            f.write(f'Epoch {epoch+1}/{epochs}, Train Loss: {trn_loss:.4f}, Val Loss: {val_loss:.4f}\\n')\n",
    "        \n",
    "    xm.rendezvous('save_model')\n",
    "    xm.master_print('save model')\n",
    "    xm.save(model.state_dict(), f'./Model_files/xla_trained_model_epoch.pth')\n",
    "\n",
    "def _mp_fn(rank, flags):\n",
    "    try:\n",
    "        model = mae_vit_base_patch16_dec512d8b(img_size=125, mask_ratio=0.75)\n",
    "        dev = xm.xla_device()\n",
    "        model = model.to(dev)\n",
    "        dataloader_train, dataloader_test = data()\n",
    "        torch.set_default_tensor_type('torch.FloatTensor')\n",
    "        train_function(model, epochs=50)\n",
    "        # xser.save(model.state_dict(), f\"model.bin\", master_only=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Exception in process {rank}: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    FLAGS = {}\n",
    "    try:\n",
    "        xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')\n",
    "    except Exception as e:\n",
    "        print(f\"Exception in main: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88499da3-415e-4d36-8e8f-068af833b849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _mp_fn(rank, flags):\n",
    "#     model = mae_vit_base_patch16_dec512d8b(img_size=125, mask_ratio = 0.75)\n",
    "#     train_function(model, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb4a05f-3fed-415f-a0ac-c8d6cd13a77f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     FLAGS={}\n",
    "#     xmp.spawn(_mp_fn, args=(FLAGS,), start_method='fork')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77142c88-54c8-46de-b6ef-ae2379f574f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xm.save(model.encoder.state_dict(), './Model_files/encoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dac7c5b-0d1b-4554-9365-b298118c5e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./Model_files/xla_trained_model_epoch.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fe10de-f1d9-4f6a-a60b-b0c18699311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = xm.xla_device()\n",
    "imgs, pred, ind = model(sample['img'].unsqueeze(0).to(dev))\n",
    "\n",
    "pred.shape\n",
    "\n",
    "def unpatchify(x):\n",
    "    \"\"\"\n",
    "    x: (N, L, patch_size**2 *3)\n",
    "    imgs: (N, 8, H, W)\n",
    "    \"\"\"\n",
    "    p = 5\n",
    "    h = w = int(x.shape[1]**.5)\n",
    "    assert h * w == x.shape[1]\n",
    "    \n",
    "    x = x.reshape(shape=(x.shape[0], h, w, p, p, 8))\n",
    "    x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "    imgs = x.reshape(shape=(x.shape[0], 8, h * p, h * p))\n",
    "    return imgs\n",
    "\n",
    "# pred = pred.reshape((125,125,8))\n",
    "pred = unpatchify(pred)\n",
    "pred.shape\n",
    "pred = pred.reshape((8, 125, 125))\n",
    "pred.shape\n",
    "\n",
    "img = pred.permute(1,2,0)\n",
    "img = img.cpu().detach().numpy()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(8):\n",
    "    plt.imshow(img[:,:,i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc70ce18-a2e4-4938-ab1b-9fdc09d0f00c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb32d26-7716-4450-bc06-68037fd0015c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36cd16-89c0-4316-ad10-c24001c7dddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab8dec0-5df8-4137-9bd4-1ff821c71b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DeepL)",
   "language": "python",
   "name": "deepl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

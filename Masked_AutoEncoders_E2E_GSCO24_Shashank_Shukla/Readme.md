<h1 align="center">
Masked Auto-Encoder for Efficient End-to-End Particle Reconstruction and Compression</br>
</h1>

This work is a part of the Google Summer of Code (GSoC) 2024 program under the Machine Learning for Science (ML4SCI) initiative.
<p align="center">
  <img src="https://github.com/Wodlfvllf/E2E/blob/main/Masked_AutoEncoders_E2E_GSCO24_Shashank_Shukla/images/gsoc1.png" width="700" title="hover text">
</p>

## Abstract
This project implements a Masked Autoencoder (MAE) using Vision Transformers (ViTs) to reconstruct particle detector images and compress data from the CMS open data. The goal is to develop efficient models for data compression and precise particle identification, which are crucial in handling the massive amounts of data generated by particle collisions in high-energy physics.

## Introduction
In high-energy physics experiments like those at the CMS detector in the LHC, classifying particle collisions is essential for studying fundamental forces and searching for phenomena beyond the Standard Model. Traditional methods, such as particle flow algorithms, have been used for event classification, but machine learning (ML) approaches like end-to-end reconstruction have recently shown promise. Image representations with CNNs and Vision Transformers (ViTs) have been employed to capture complex patterns in data. Using Masked Image Modeling (MIM) with masked autoencoders, this study leverages self-supervised learning to classify quark jet images more efficiently, reducing the reliance on labeled datasets.

<p align="center">
  <img src="https://github.com/Wodlfvllf/E2E/blob/main/Masked_AutoEncoders_E2E_GSCO24_Shashank_Shukla/images/MAE%20(1).jpg" width="700" title="hover text">
</p>

## Data Pre-processing
The dataset, sourced from the CMS Open Data collection, consists of simulated particle collision data generated with Madgraph 2.6.6 and Pythia6 for parton showering. It includes around 4 million images (~2 million top quark jets and ~2 million non-top quark jets) with transverse momentum (pT) greater than 400 GeV. The sample contains approximately 4 million images (~2mil. signal, ~2mil. background). The images are obtained by cropping a window along ieta and iphi (indexed positions in the detector) around a candidate jet. Each image contains 8 distinct channels: the track transverse momentum, the longitudinal impact parameter, the transverse impact parameter, electron calorimeter energy deposition, hadronic calorimeter energy deposition, and barrel pixel layer energy deposition.

The data is further pre-processed by the following: 
1) pixel values less than 1e-3 are suppressed to 0;
2) each channel is separately z-score normalized;
3) each channel is clipped outside of 500 times the standard deviation of the pixel value;
4) each channel is min-max scaled.

## Training
All models were trained on A100 GPUs using PyTorch with data in H5 format. The dataset consisted of 3.9 million events, with 2 million used for pre-training, 1 million for fine-tuning and linear probing, and the remaining 800k events split equally between test and validation (400k each). Pre-training lasted 47 hours on two A100 GPUs over 80-100 epochs, while fine-tuning was done for 10 epochs using the AdamW optimizer with a learning rate of 1.5e-4.

## Results

### **Visualizing Image Reconstruction**

#### **Original**
<p align="center">
  <img src="https://github.com/Wodlfvllf/End-to-End-Deep-Learning-Project/blob/main/Specific%20Task%203d%20-%20Masked_Autoencoder/Original.jpg" width="700" title="Original Image">
</p>

#### **Reconstructed**
<p align="center">
  <img src="https://github.com/Wodlfvllf/End-to-End-Deep-Learning-Project/blob/main/Specific%20Task%203d%20-%20Masked_Autoencoder/Reconstructed.jpg" width="700" title="Reconstructed Image">
</p>

### **Performance Table**

#### **Linear Probing**
<p align="center">
  <img src="https://github.com/Wodlfvllf/E2E/blob/main/Masked_AutoEncoders_E2E_GSCO24_Shashank_Shukla/images/LinearProbing%20Table.png" width="700" title="Linear Probing Table">
</p>    

####                                            FineTuning
<p align="center">
  <img src="https://github.com/Wodlfvllf/E2E/blob/main/Masked_AutoEncoders_E2E_GSCO24_Shashank_Shukla/images/Finetune.png" width="700" title="hover text">
</p>   

The experimental results demonstrate that the Base MAE model outperformed ResNet-15 during fine-tuning. Specifically, the Base MAE achieved an accuracy of 0.9306 and an AUC-ROC score of 0.9830, compared to ResNet-15's AUC-ROC of 0.9824. The Depthwise Convolution MAE model also exhibited competitive performance, achieving a higher accuracy of 0.9376 but a slightly lower AUC-ROC score of 0.9816.

Both the Base MAE and Depthwise Convolution models demonstrated high parameter efficiency during linear probing, each with only 385 trainable parameters. They achieved accuracy and AUC-ROC scores close to those of ResNet-15, showing that the MAE models captured high-quality representations during pre-training, requiring minimal retraining for classification tasks.

## Notebooks
All the notebooks can be found here 
- [Notebooks](https://github.com/Wodlfvllf/E2E/tree/main/Masked_AutoEncoders_E2E_GSCO24_Shashank_Shukla/Notebooks)
- [Example_train](https://github.com/Wodlfvllf/E2E/blob/main/Masked_AutoEncoders_E2E_GSCO24_Shashank_Shukla/train_example.ipynb)

## Run python scripts in DDP

### Example Script
```
srun torchrun --standalone --nproc_per_node=2 path-to-train-script \
                --epochs=100 \
                --batch_size=256 \
                --model_name="conv_mae" \
                --learning_rate=0.00015 \
                --train_samples=20000 \
                --patch_size=4 \
                --embed_dim=64 \
                --num_heads=8 \
                --decoder_embed_dim=256 \
                --decoder_depth=8 \
                --data_path='path-to-dataset' \
                --warmup=3
```

## Refer to this blog for details of the project.
[Masked Auto-Encoders](https://medium.com/@shuklashashankshekhar863/masked-autoencoders-for-efficient-end-to-end-particle-reconstruction-and-compression-for-the-cms-fdd7b941a2bb)
## Dependencies
- Python 3.x
- Jupyter Notebook
- PyTorch
- NumPy
- Pandas
- Matplotlib

Install these dependencies using pip or conda.

# Masked Auto-Encoder for Efficient End-to-End Particle Reconstruction and Compression
This project implements a Masked Autoencoder (MAE) using Vision Transformers (ViTs) to reconstruct particle detector images and compress data from the CMS open data. The goal is to develop efficient models for data compression and precise particle identification, which are crucial in handling the massive amounts of data generated by particle collisions in high-energy physics.

This work is a part of the Google Summer of Code (GSoC) 2024 program under the Machine Learning for Science (ML4SCI) initiative.

### Tasks
1. Train a lightweight ViT using the Masked Auto-Encoder (MAE) training scheme on the unlabelled dataset.
2. Compare reconstruction results using MAE on both training and testing datasets.
3. Fine-tune the model at a lower learning rate on the provided labeled dataset and compare results with those of a model trained from scratch.

<p align="center">
  <img src="https://github.com/Wodlfvllf/E2E/blob/main/Masked_AutoEncoders_E2E_GSCO24_Shashank_Shukla/MAE%20(1).jpg" width="700" title="hover text">
</p>

### Implementation
- Trained a lightweight ViT MAE on the unlabelled dataset
- Compared reconstruction results on training and testing datasets
- Fine-tuned the model on a lower learning rate using the labeled dataset
- Compared results with baselines such as ResNets.

### Image Reconstruction
####                                           Original
<p align="center">
  <img src="https://github.com/Wodlfvllf/End-to-End-Deep-Learning-Project/blob/main/Specific%20Task%203d%20-%20Masked_Autoencoder/Original.jpg" width="700" title="hover text">
</p>

####                                          Reconstructed
<p align="center">
  <img src="https://github.com/Wodlfvllf/End-to-End-Deep-Learning-Project/blob/main/Specific%20Task%203d%20-%20Masked_Autoencoder/Reconstructed.jpg" width="700" title="hover text">
</p>

### Comparison of Different Masked AutoEncoder Architectures

####                                           Linear Probing
<p align="center">
  <img src="https://github.com/Wodlfvllf/E2E/blob/main/Masked_AutoEncoders_E2E_GSCO24_Shashank_Shukla/LinearProbing%20Table.png" width="700" title="hover text">
</p>       
In Linear probing Base-Mae model achieved the highest accuracy of 0.8958 and 0.9747 AUC Score.

####                                            FineTuning
<p align="center">
  <img src="https://github.com/Wodlfvllf/E2E/blob/main/Masked_AutoEncoders_E2E_GSCO24_Shashank_Shukla/Finetune.png" width="700" title="hover text">
</p>      
In Fine-tuning Base-Mae model achieved the highest accuracy of 0.9306 and 0.983 AUC Score while Depth

## Refer to this blog for details of the project.
[Masked Auto-Encoders](https://medium.com/@shuklashashankshekhar863/masked-autoencoders-for-efficient-end-to-end-particle-reconstruction-and-compression-for-the-cms-fdd7b941a2bb)
## Dependencies
- Python 3.x
- Jupyter Notebook
- PyTorch
- NumPy
- Pandas
- Matplotlib

Install these dependencies using pip or conda.

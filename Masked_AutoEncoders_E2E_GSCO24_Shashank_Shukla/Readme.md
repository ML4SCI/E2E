<h1 align="center">
Masked Auto-Encoder for Efficient End-to-End Particle Reconstruction and Compression</br>
</h1>

This work is a part of the Google Summer of Code (GSoC) 2024 program under the Machine Learning for Science (ML4SCI) initiative.
<p align="center">
  <img src="https://github.com/Wodlfvllf/E2E/blob/main/Masked_AutoEncoders_E2E_GSCO24_Shashank_Shukla/gsoc1.png" width="700" title="hover text">
</p>

## Abstract
This project implements a Masked Autoencoder (MAE) using Vision Transformers (ViTs) to reconstruct particle detector images and compress data from the CMS open data. The goal is to develop efficient models for data compression and precise particle identification, which are crucial in handling the massive amounts of data generated by particle collisions in high-energy physics.

## Introduction
In high-energy physics experiments like those at the CMS detector in the LHC, classifying particle collisions is essential for studying fundamental forces and searching for phenomena beyond the Standard Model. Traditional methods, such as particle flow algorithms, have been used for event classification, but machine learning (ML) approaches like end-to-end reconstruction have recently shown promise. Image representations with CNNs and Vision Transformers (ViTs) have been employed to capture complex patterns in data. Using Masked Image Modeling (MIM) with masked autoencoders, this study leverages self-supervised learning to classify quark jet images more efficiently, reducing the reliance on labeled datasets.

<p align="center">
  <img src="https://github.com/Wodlfvllf/E2E/blob/main/Masked_AutoEncoders_E2E_GSCO24_Shashank_Shukla/MAE%20(1).jpg" width="700" title="hover text">
</p>

## Data Pre-processing
ViTs are very sensitive to data pre-processing techniques. It has often been seen that even things like Image Interpolation Techniques, if not done properly, can adversely affect the performance of Transformers-based models. In our case, the data is directly sourced from CMS Open Data, and the outputs(pixel) can be arbitrarily large for a single detector(calorimeter) hence proper normalization techniques are employed.
We employ the following steps to ensure that the smaples or our datapoints are properly normalised and free from outliers:
* Zero suppression for any value under 10^-3.
* Channel Wise Z-score Normalisation across batches of size 4096.
* Channel Wise Max value clipping with clip value set equal to 500 times the standard deviation of the pixel values.
* Sample wise Min Max scaling.
Although the pre-processing is same for both the datasets but the input pipelines are vastly difference due to the training environment and computational challenges.

## Training
All models were trained on A100 GPUs using PyTorch with data in H5 format. The dataset consisted of 3.9 million events, with 2 million used for pre-training, 1 million for fine-tuning and linear probing, and the remaining 800k events split equally between test and validation (400k each). Pre-training lasted 47 hours on two A100 GPUs over 80-100 epochs, while fine-tuning was done for 10 epochs using the AdamW optimizer with a learning rate of 1.5e-4.

## Results

### Visualising Image Reconstruction
####                                           Original
<p align="center">
  <img src="https://github.com/Wodlfvllf/End-to-End-Deep-Learning-Project/blob/main/Specific%20Task%203d%20-%20Masked_Autoencoder/Original.jpg" width="700" title="hover text">
</p>

####                                          Reconstructed
<p align="center">
  <img src="https://github.com/Wodlfvllf/End-to-End-Deep-Learning-Project/blob/main/Specific%20Task%203d%20-%20Masked_Autoencoder/Reconstructed.jpg" width="700" title="hover text">
</p>

### Performance Table

####                                           Linear Probing
<p align="center">
  <img src="https://github.com/Wodlfvllf/E2E/blob/main/Masked_AutoEncoders_E2E_GSCO24_Shashank_Shukla/LinearProbing%20Table.png" width="700" title="hover text">
</p>       
In Linear probing Base-Mae model achieved the highest accuracy of 0.9035 and 0.9747 AUC Score.

####                                            FineTuning
<p align="center">
  <img src="https://github.com/Wodlfvllf/E2E/blob/main/Masked_AutoEncoders_E2E_GSCO24_Shashank_Shukla/Finetune.png" width="700" title="hover text">
</p>      
In Fine-tuning Base-Mae model achieved the highest accuracy of 0.9306 and 0.983 AUC Score while Depth

## Refer to this blog for details of the project.
[Masked Auto-Encoders](https://medium.com/@shuklashashankshekhar863/masked-autoencoders-for-efficient-end-to-end-particle-reconstruction-and-compression-for-the-cms-fdd7b941a2bb)
## Dependencies
- Python 3.x
- Jupyter Notebook
- PyTorch
- NumPy
- Pandas
- Matplotlib

Install these dependencies using pip or conda.

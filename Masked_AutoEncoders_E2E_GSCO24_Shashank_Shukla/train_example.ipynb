{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f7de17c-a6ec-4d14-9018-a70c7168a7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep 22 06:30:21 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:03:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    50W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48271d00-e012-48f9-a975-0e40a0558369",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/global/u2/s/ssshukla/Shashank'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfda7ba4-ce3f-4b8e-a622-45e86438ea4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/u2/s/ssshukla/Shashank/scripts/LinearProbing\n"
     ]
    }
   ],
   "source": [
    "%cd ./scripts/LinearProbing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31fdd617-7e65-4d45-a39c-2c100d6d6581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from util import *\n",
    "from data import *\n",
    "from model import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88cd7c2f-a64f-4419-a6b7-cb336da25289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from train import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e0c8cc9-4f3c-4532-8cbc-10f8ab4fec4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('Masked Autoencoder ViT', add_help=False, allow_abbrev=False)\n",
    "\n",
    "    # Model related arguments\n",
    "    parser.add_argument('--model_name', default=\"base_mae_depthwise_convolution\", choices=[\"base_mae_depthwise_convolution\",\n",
    "                                                                                           \"channel_former\",\n",
    "                                                                                           \"base_mae\",\n",
    "                                                                                           \"conv_mae\",\n",
    "                                                                                           \"cross_vit\"],type=str, help='Model architecture to train')\n",
    "    parser.add_argument('--img_size', default=125, type=int, help='Image size')\n",
    "    parser.add_argument('--patch_size', default=5, type=int, help='Patch size')\n",
    "    parser.add_argument('--in_chans', default=8, type=int, help='Number of input channels')\n",
    "    parser.add_argument('--embed_dim', default=128, type=int, help='Embedding dimension')\n",
    "    parser.add_argument('--depth', default=16, type=int, help='Depth of the encoder')\n",
    "    parser.add_argument('--num_heads', default=8, type=int, help='Number of attention heads')\n",
    "    parser.add_argument('--k_factor', default=16, type=int, help='Factor for convolution projection')\n",
    "\n",
    "    # Decoder related arguments\n",
    "    parser.add_argument('--decoder_embed_dim', default=128, type=int, help='Decoder embedding dimension')\n",
    "    parser.add_argument('--decoder_depth', default=8, type=int, help='Decoder depth')\n",
    "    parser.add_argument('--decoder_num_heads', default=8, type=int, help='Number of decoder heads')\n",
    "\n",
    "    # Other arguments\n",
    "    parser.add_argument('--mask_ratio', default=0.75, type=float, help='Masking ratio')\n",
    "    parser.add_argument('--norm_layer', default=nn.LayerNorm, type=str, help='Normalization layer')\n",
    "    parser.add_argument('--mlp_ratio', default=4, type=float, help='MLP ratio')\n",
    "    parser.add_argument('--batch_size', default=128, type=int, help='Batch size')\n",
    "    parser.add_argument('--learning_rate', default=0.00001, type=float, help='learning rate')\n",
    "    parser.add_argument('--epochs', default=1, type=int, help='epochs')\n",
    "    parser.add_argument('--save_every', default=1, type=int, help='How often to save a snapshot')\n",
    "    parser.add_argument('--train_samples', default=-1, type=int, help='-1 indicates use samples for training')\n",
    "    parser.add_argument('--resume_training', default=False, type=bool, help='Weather to resume from a checkpoint')\n",
    "    parser.add_argument('--data_path', default='/pscratch/sd/s/ssshukla/Boosted_Top.h5', type=str, help='Path to the dataset')\n",
    "    parser.add_argument('--warmup', type=int, default=3, ###This should be ~5-10% of total epochs\n",
    "                        help='number of warmup epochs before reaching base_lr')\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72df5561-f2c6-4ff9-b065-3a32b96684a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = get_args_parser()\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7260b25-62cc-40c6-8eed-d2872da96bcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = get_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09758fad-bc32-431d-aa66-aabeda41934c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = H5MaskedAutoEncoderDataset(h5_path = args.data_path, preload_size = args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c595a85d-198f-4a4a-b77a-66a45e546053",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=args.batch_size,\n",
    "                        num_workers = 8,\n",
    "                        pin_memory=True,\n",
    "                        shuffle=True,\n",
    "                        drop_last=True \n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc46f1d2-1d01-4c48-a042-34f5d3d1f0c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for step, batch in enumerate(dataloader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a1e9b4-c1df-4c48-b205-73c0eaf6a913",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 (Train):   0%|          | 16/24725 [01:20<9:28:51,  1.38s/batch] "
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "def model_train(model, epochs, train_dataloader, device):\n",
    "    # Define optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=1.5e-4, weight_decay=0.05)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=40, T_mult=2)\n",
    "\n",
    "    # Lists to store training losses\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0  # To accumulate training loss\n",
    "\n",
    "        model.train()  # Set model to training mode\n",
    "        for batch in tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{epochs} (Train)', unit='batch'):\n",
    "            images = batch['img'].to(device)  # Send images to device (GPU/CPU)\n",
    "            \n",
    "            optimizer.zero_grad()  # Zero out gradients\n",
    "            loss, outputs, mask = model(images)  # Forward pass through the model\n",
    "            loss = loss.sum()  # Sum the loss over the batch\n",
    "            \n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Optimizer step\n",
    "            \n",
    "            train_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "        # Average training loss over the entire dataset\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Step the scheduler after each epoch\n",
    "        scheduler.step()\n",
    "\n",
    "        # Logging training loss\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}')\n",
    "        \n",
    "        # Save the loss to a file\n",
    "        with open('losses.txt', 'a') as f:  # Open file in append mode\n",
    "            f.write(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}\\n')\n",
    "        \n",
    "        # Save the model after each epoch\n",
    "        torch.save(model.state_dict(), f'./full_model_epoch_{epoch+1}.pth')\n",
    "\n",
    "    # Return the list of training losses\n",
    "    return train_losses\n",
    "\n",
    "# Clean up memory (if needed)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Set the device to use\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assume the model and dataloader are already defined\n",
    "model = model.to(DEVICE)  # Move the model to the appropriate device (GPU/CPU)\n",
    "\n",
    "# Train the model\n",
    "train_losses = model_train(model, 1, dataloader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ff0fcb-e294-4e64-b14f-bed29718ecae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.3.1",
   "language": "python",
   "name": "pytorch-2.3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
